
=== High Availability image:../images/edit.png[link="https://github.com/ArcadeData/arcadedb-docs/blob/main/src/main/asciidoc/server/ha.adoc" float="right"]

ArcadeDB supports a High Availability mode where multiple servers share the same database (replication).

To start ArcadeDB Server in High Availability (HA) mode, modify the default setting <<#_settings,`ha.enabled`>> to `true` and the server will listen for incoming connections from other nodes. If any other servers are already started, then the current server will join the cluster. Example:

```shell
~/arcadedb $ bin/server.sh -Darcadedb.ha.enabled=true

<ArcadeDB_0> Starting ArcadeDB Server... [ArcadeDBServer]
<ArcadeDB_0> - JMX Metrics Started... [ArcadeDBServer]
<ArcadeDB_0> - Starting HTTP Server (host=0.0.0.0 port=2480)... [HttpServer]
XNIO version 3.3.8.Final [xnio]
XNIO NIO Implementation Version 3.3.8.Final [nio]
<ArcadeDB_0> - HTTP Server started (host=0.0.0.0 port=2480) [HttpServer]
<ArcadeDB_0> Listening Replication connections on 127.0.0.1:2424 (protocol v.-1) [LeaderNetworkListener]
<ArcadeDB_0> Unable to find any Leader, start election (cluster=arcadedb configuredServers=1 majorityOfVotes=1) [HAServer]
<ArcadeDB_0> Change election status from DONE to VOTING_FOR_ME [HAServer]
<ArcadeDB_0> ArcadeDB Server started (CPUs=8 MAXRAM=1.92GB) [ArcadeDBServer]
<ArcadeDB_0> Starting election of local server asking for votes from [] (turn=1 retry=0 lastReplicationMessage=-1 configuredServers=1 majorityOfVotes=1) [HAServer]
<ArcadeDB_0> Current server elected as new Leader (turn=1 totalVotes=1 majority=1) [HAServer]
<ArcadeDB_0> Change election status from VOTING_FOR_ME to LEADER_WAITING_FOR_QUORUM [HAServer]
<ArcadeDB_0> Contacting all the servers for the new leadership (turn=1)... [HAServer]
```

==== Architecture

ArcadeDB has a Leader/Replica model by using RAFT consensus for election and replication.

[ditaa,ha-architecture]
....
 +------------+        +------------+        +------------+
 | ArcadeDB_1 |<-------| ArcadeDB_0 |------->| ArcadeDB_2 |
 |  Replica   |        |   Leader   |        |  Replica   |
 +------------+        +------------+        +------------+
       |                     |                     |
       |                     |                     |
       V                     V                     V
  +----------+          +----------+          +----------+
  |  Journal |          |  Journal |          |  Journal |
  |      {d} |          |      {d} |          |      {d} |
  +----------+          +----------+          +----------+
....

Each server has its own Journal. The Journal is used in case of recovery of the cluster to get the most updated replica and to align the other nodes. All the write operations must be coordinated by the Leader node.


In case a client is connected to a Replica, all the read requests will be executed by the replica, but any write requests will be transparently forwarded to the Leader. Everything is transparent for the end user.

[ditaa,ha-architecture]
....
 +------------+
 |   Client   |
 +-----+------+
       |
       V
 +------------+        +------------+
 | ArcadeDB_1 |------->| ArcadeDB_0 |
 |  Replica   |forward |   Leader   |
 +------------+        +------------+
....



More coming soon.

==== Starting multiple servers in cluster

More coming soon.

==== Auto fail-over

More coming soon.

==== Auto balancing clients

More coming soon.

