[[appendix-graph-algorithms]]
[appendix]
== Graph Algorithms
image:../images/edit.png[link="https://github.com/ArcadeData/arcadedb-docs/blob/main/src/main/asciidoc/appendix/graph-algorithms.adoc" float=right]

ArcadeDB provides a comprehensive library of graph algorithms available as Cypher procedures and SQL functions. Cypher procedures follow the pattern:

[source,cypher]
----
CALL algo.name([arg1, arg2, ...]) YIELD field1, field2, ...
RETURN field1, field2, ...
----

SQL path functions return a list of vertex RIDs and can be used directly in `SELECT` statements.

=== Algorithm Index

[cols="3,2,2,1", options="header"]
|===
| Algorithm | Procedure / Function | Category | Complexity

| <<algo-astar,A*>> | `algo.astar` | Path Finding | CPU
| <<algo-adamic-adar,Adamic-Adar Index>> | `algo.adamicAdar` | Link Prediction | CPU
| <<algo-apsp,All-Pairs Shortest Paths (APSP)>> | `algo.apsp` | Path Finding | CPU+RAM
| <<algo-all-simple-paths,All Simple Paths>> | `algo.allsimplepaths` | Path Finding | CPU
| <<algo-article-rank,ArticleRank>> | `algo.articlerank` | Centrality | CPU
| <<algo-articulation-points,Articulation Points>> | `algo.articulationPoints` | Structural Analysis | CPU
| <<algo-assortativity,Assortativity>> | `algo.assortativity` | Network Science | CPU
| <<algo-bellman-ford,Bellman-Ford>> | `algo.bellmanford` | Path Finding | CPU
| <<algo-bellman-ford-sql,Bellman-Ford (SQL)>> | `bellmanFord()` | Path Finding | CPU
| <<algo-betweenness,Betweenness Centrality>> | `algo.betweenness` | Centrality | CPU
| <<algo-bfs,BFS (Breadth-First Search)>> | `algo.bfs` | Path Finding | CPU
| <<algo-biconnected-components,Biconnected Components>> | `algo.biconnectedComponents` | Structural Analysis | CPU
| <<algo-bipartite,Bipartite Check>> | `algo.bipartite` | Structural Analysis | CPU
| <<algo-bipartite-matching,Bipartite Matching>> | `algo.bipartiteMatching` | Structural Analysis | CPU
| <<algo-bridges,Bridges>> | `algo.bridges` | Structural Analysis | CPU
| <<algo-clique,Clique Enumeration>> | `algo.clique` | Structural Analysis | CPU
| <<algo-closeness,Closeness Centrality>> | `algo.closeness` | Centrality | CPU
| <<algo-common-neighbors,Common Neighbors>> | `algo.commonNeighbors` | Link Prediction | CPU
| <<algo-conductance,Conductance>> | `algo.conductance` | Community Quality | CPU
| <<algo-cycle-detection,Cycle Detection>> | `algo.cycleDetection` | Structural Analysis | CPU
| <<algo-degree,Degree Centrality>> | `algo.degree` | Centrality | CPU
| <<algo-densest-subgraph,Densest Subgraph>> | `algo.densestSubgraph` | Structural Analysis | CPU
| <<algo-dfs,DFS (Depth-First Search)>> | `algo.dfs` | Path Finding | CPU
| <<algo-dijkstra,Dijkstra>> | `algo.dijkstra` | Path Finding | CPU
| <<algo-dijkstra-single-source,Dijkstra Single-Source>> | `algo.dijkstra.singleSource` | Path Finding | CPU
| <<algo-dijkstra-sql,Dijkstra (SQL)>> | `dijkstra()` | Path Finding | CPU
| <<algo-eccentricity,Eccentricity>> | `algo.eccentricity` | Centrality | CPU
| <<algo-eigenvector,Eigenvector Centrality>> | `algo.eigenvector` | Centrality | CPU
| <<algo-graph-coloring,Graph Coloring>> | `algo.graphColoring` | Structural Analysis | CPU
| <<algo-graph-summary,Graph Summary>> | `algo.graphSummary` | Graph Statistics | CPU
| <<algo-harmonic,Harmonic Centrality>> | `algo.harmonic` | Centrality | CPU
| <<algo-hierarchical-clustering,Hierarchical Clustering>> | `algo.hierarchicalClustering` | Community Detection | CPU+RAM
| <<algo-hits,HITS (Hubs and Authorities)>> | `algo.hits` | Centrality | CPU
| <<algo-influence-maximization,Influence Maximization>> | `algo.influenceMaximization` | Network Science | CPU
| <<algo-fastrp,FastRP (Fast Random Projection)>> | `algo.fastrp` | Node Embedding | CPU
| <<algo-graphsage,GraphSAGE>> | `algo.graphsage` | Node Embedding | CPU
| <<algo-hashgnn,HashGNN>> | `algo.hashgnn` | Node Embedding | CPU
| <<algo-jaccard,Jaccard Similarity>> | `algo.jaccard` | Similarity | CPU
| <<algo-k-core,K-Core Decomposition>> | `algo.kcore` | Structural Analysis | CPU
| <<algo-knn,K-Nearest Neighbours (KNN)>> | `algo.knn` | Similarity | CPU
| <<algo-k-shortest-paths,K-Shortest Paths>> | `algo.kShortestPaths` | Path Finding | CPU+RAM
| <<algo-k-truss,K-Truss Decomposition>> | `algo.kTruss` | Structural Analysis | CPU
| <<algo-katz,Katz Centrality>> | `algo.katz` | Centrality | CPU
| <<algo-label-propagation,Label Propagation>> | `algo.labelpropagation` | Community Detection | CPU
| <<algo-leiden,Leiden>> | `algo.leiden` | Community Detection | CPU
| <<algo-local-clustering-coefficient,Local Clustering Coefficient>> | `algo.localClusteringCoefficient` | Structural Analysis | CPU
| <<algo-longest-path-dag,Longest Path (DAG)>> | `algo.longestPath` | Path Finding | CPU
| <<algo-louvain,Louvain>> | `algo.louvain` | Community Detection | CPU
| <<algo-max-k-cut,Max k-Cut (Approximate)>> | `algo.maxKCut` | Community Detection | CPU
| <<algo-max-flow,Maximum Flow>> | `algo.maxFlow` | Network Flow | CPU+RAM
| <<algo-msa,Minimum Spanning Arborescence>> | `algo.msa` | Path Finding | CPU
| <<algo-mst,Minimum Spanning Tree (MST)>> | `algo.mst` | Structural Analysis | CPU
| <<algo-modularity-score,Modularity Score>> | `algo.modularityScore` | Community Quality | CPU
| <<algo-node2vec,Node2Vec>> | `algo.node2vec` | Node Embedding | CPU
| <<algo-page-rank,PageRank>> | `algo.pagerank` | Centrality | CPU
| <<algo-personalized-page-rank,Personalized PageRank>> | `algo.personalizedPageRank` | Centrality | CPU
| <<algo-preferential-attachment,Preferential Attachment>> | `algo.preferentialAttachment` | Link Prediction | CPU
| <<algo-random-walk,Random Walk>> | `algo.randomWalk` | Traversal / Sampling | CPU
| <<algo-resource-allocation,Resource Allocation>> | `algo.resourceAllocation` | Link Prediction | CPU
| <<algo-rich-club,Rich-Club Coefficient>> | `algo.richClub` | Network Science | CPU
| <<algo-same-community,Same Community>> | `algo.sameCommunity` | Link Prediction | CPU
| <<algo-shortest-path-sql,Shortest Path (SQL)>> | `shortestPath()` | Path Finding | CPU
| <<algo-sim-rank,SimRank>> | `algo.simRank` | Similarity | CPU+RAM
| <<algo-slpa,SLPA (Speaker-Listener Label Propagation)>> | `algo.slpa` | Community Detection | CPU
| <<algo-scc,Strongly Connected Components (SCC)>> | `algo.scc` | Community Detection | CPU
| <<algo-steiner-tree,Steiner Tree>> | `algo.steinerTree` | Path Finding | CPU+RAM
| <<algo-topological-sort,Topological Sort>> | `algo.topologicalSort` | Traversal / Ordering | CPU
| <<algo-total-neighbors,Total Neighbors>> | `algo.totalNeighbors` | Link Prediction | CPU
| <<algo-triangle-count,Triangle Count>> | `algo.triangleCount` | Structural Analysis | CPU
| <<algo-vote-rank,VoteRank>> | `algo.voteRank` | Centrality | CPU
| <<algo-wcc,Weakly Connected Components (WCC)>> | `algo.wcc` | Community Detection | CPU
|===

=== Path Finding

[[algo-dijkstra]]
==== algo.dijkstra

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.dijkstra`
| Category | Path Finding
| Complexity | CPU
| Min Args | 4
| Max Args | 5
|===

*Syntax*

[source,cypher]
----
CALL algo.dijkstra(startNode, endNode, relTypes, weightProperty [, direction])
YIELD path, weight
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex
| `endNode` | Vertex | Yes | — | Destination vertex
| `relTypes` | String | Yes | — | Comma-separated relationship types; pass `''` for all types
| `weightProperty` | String | Yes | — | Edge property name to use as weight
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `path` | List<RID> | Ordered list of vertex identities on the shortest path
| `weight` | Double | Total path weight
|===

*Description*

Finds the single shortest weighted path between two vertices using Dijkstra's algorithm. Non-negative edge weights are required. Internally delegates to the A* implementation with no heuristic.

*Use Cases*

- Route planning in transportation networks
- Cost-optimal navigation in weighted graphs
- Network latency analysis

*Example*

[source,cypher]
----
MATCH (src:City {name:'Rome'}), (dst:City {name:'Berlin'})
CALL algo.dijkstra(src, dst, 'ROAD', 'distance', 'OUT')
YIELD path, weight
RETURN path, weight
----

*References*

- https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm[Dijkstra's algorithm – Wikipedia]

See also: <<algo-dijkstra-sql>> for the SQL function variant, and <<astar,astar()>> / <<dijkstra,dijkstra()>> for the SQL function equivalents.

'''

[[algo-astar]]
==== algo.astar

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.astar`
| Category | Path Finding
| Complexity | CPU
| Min Args | 4
| Max Args | 6
|===

*Syntax*

[source,cypher]
----
CALL algo.astar(startNode, endNode, relTypes, weightProperty [, latProperty, lonProperty])
YIELD path, weight
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex
| `endNode` | Vertex | Yes | — | Destination vertex
| `relTypes` | String | Yes | — | Comma-separated relationship types; pass `''` for all types
| `weightProperty` | String | Yes | — | Edge property name to use as weight
| `latProperty` | String | No | — | Vertex property name for latitude (enables geographic heuristic)
| `lonProperty` | String | No | — | Vertex property name for longitude (enables geographic heuristic)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `path` | List<RID> | Ordered list of vertex identities on the shortest path
| `weight` | Double | Total path weight
|===

*Description*

Finds the shortest weighted path using the A* algorithm. When `latProperty` and `lonProperty` are provided, a geographic (Euclidean) heuristic accelerates the search. Without the geographic properties it behaves identically to Dijkstra.

*Use Cases*

- Geospatial route optimization
- Game AI pathfinding with spatial heuristics
- Weighted graph shortest paths with domain-specific heuristics

*Example*

[source,cypher]
----
MATCH (src:City {name:'Milan'}), (dst:City {name:'Paris'})
CALL algo.astar(src, dst, 'ROAD', 'km', 'lat', 'lon')
YIELD path, weight
RETURN path, weight
----

*References*

- https://en.wikipedia.org/wiki/A*_search_algorithm[A* search algorithm – Wikipedia]

'''

[[algo-bellman-ford]]
==== algo.bellmanford

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.bellmanford`
| Category | Path Finding
| Complexity | CPU
| Min Args | 4
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.bellmanford(startNode, endNode, relTypes, weightProperty)
YIELD path, weight, negativeCycle
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex
| `endNode` | Vertex | Yes | — | Destination vertex
| `relTypes` | String | Yes | — | Comma-separated relationship types; pass `''` for all types
| `weightProperty` | String | Yes | — | Edge property name to use as weight (may be negative)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `path` | List<RID> | Ordered list of vertex identities on the shortest path
| `weight` | Double | Total path weight
| `negativeCycle` | Boolean | `true` if a negative-weight cycle is reachable from `startNode`
|===

*Description*

Finds the shortest weighted path between two vertices using the Bellman-Ford algorithm. Unlike Dijkstra, it correctly handles graphs with negative edge weights. It runs V−1 relaxation iterations followed by one extra iteration to detect negative-weight cycles. If a negative cycle is detected, `negativeCycle` is set to `true` and the path result is unreliable.

*Use Cases*

- Financial transaction networks with negative-cost edges
- Currency arbitrage detection (negative cycle = profit cycle)
- Graphs where edge weights can be negative

*Example*

[source,cypher]
----
MATCH (a:Account {id:1}), (b:Account {id:5})
CALL algo.bellmanford(a, b, 'TRANSFER', 'cost')
YIELD path, weight, negativeCycle
RETURN path, weight, negativeCycle
----

*References*

- https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm[Bellman-Ford algorithm – Wikipedia]

See also: <<algo-bellman-ford-sql>> for the SQL function variant.

'''

[[algo-all-simple-paths]]
==== algo.allsimplepaths

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.allsimplepaths`
| Category | Path Finding
| Complexity | CPU
| Min Args | 4
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.allsimplepaths(startNode, endNode, relTypes, maxDepth)
YIELD path
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex
| `endNode` | Vertex | Yes | — | Destination vertex
| `relTypes` | String | Yes | — | Comma-separated relationship types; pass `''` for all types
| `maxDepth` | Integer | Yes | — | Maximum path length in hops (minimum 1)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `path` | List<RID> | Ordered list of vertex identities for one simple path
|===

*Description*

Enumerates all simple (loop-free) paths between two vertices up to a maximum depth, using DFS with backtracking. Edges are traversed in both directions. Each result row contains one complete path. Use `maxDepth` to prevent combinatorial explosion in dense graphs.

*Use Cases*

- Finding all routes between two nodes in a network
- Dependency analysis between components
- Attack path enumeration in security graphs

*Example*

[source,cypher]
----
MATCH (s:Service {name:'AuthService'}), (t:Service {name:'Database'})
CALL algo.allsimplepaths(s, t, 'CALLS', 4)
YIELD path
RETURN path
----

*References*

- https://en.wikipedia.org/wiki/Path_(graph_theory)[Path (graph theory) – Wikipedia]

'''

[[algo-k-shortest-paths]]
==== algo.kShortestPaths

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.kShortestPaths`
| Category | Path Finding
| Complexity | CPU+RAM
| Min Args | 3
| Max Args | 5
|===

*Syntax*

[source,cypher]
----
CALL algo.kShortestPaths(startNode, endNode, k [, relTypes, weightProperty])
YIELD path, weight, rank
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex
| `endNode` | Vertex | Yes | — | Destination vertex
| `k` | Integer | Yes | — | Number of shortest paths to find
| `relTypes` | String | No | all types | Comma-separated relationship types
| `weightProperty` | String | No | uniform weight | Edge property name for weights
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `path` | List<RID> | Ordered list of vertex identities
| `weight` | Double | Total path weight
| `rank` | Integer | 1-based rank (1 = shortest)
|===

*Description*

Finds the k shortest loopless paths between two vertices using Yen's algorithm. Internally uses Dijkstra on a V×V weight matrix, progressively building candidate paths by spur-node deviation from previous shortest paths. Memory usage scales as O(V²) for the weight matrix.

*Use Cases*

- Resilience planning: top-k alternative routes in case of failures
- Network redundancy analysis
- Multi-criteria path exploration

*Example*

[source,cypher]
----
MATCH (s:Router {id:'R1'}), (t:Router {id:'R10'})
CALL algo.kShortestPaths(s, t, 3, 'LINK', 'latency')
YIELD path, weight, rank
RETURN rank, path, weight ORDER BY rank ASC
----

*References*

- https://en.wikipedia.org/wiki/Yen%27s_k-shortest_path_algorithm[Yen's k shortest simple paths – Wikipedia]

'''

[[algo-apsp]]
==== algo.apsp

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.apsp`
| Category | Path Finding
| Complexity | CPU+RAM
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.apsp([weightProperty, relTypes])
YIELD source, target, distance
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `weightProperty` | String | No | uniform weight | Edge property name for weights
| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `source` | RID | Source vertex identity
| `target` | RID | Target vertex identity
| `distance` | Double | Shortest distance between source and target
|===

*Description*

Computes shortest distances between all reachable pairs of distinct vertices using the Floyd-Warshall algorithm. Time complexity is O(V³); memory usage is O(V²). Only reachable pairs (finite distance, source ≠ target) are returned. Suitable for small to medium graphs.

*Use Cases*

- Graph diameter and radius computation
- Network reachability analysis
- Pre-computing all pairwise distances for downstream analytics

*Example*

[source,cypher]
----
CALL algo.apsp('weight', 'ROAD')
YIELD source, target, distance
RETURN source, target, distance ORDER BY distance DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm[Floyd-Warshall algorithm – Wikipedia]

'''

[[algo-bfs]]
==== algo.bfs

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.bfs`
| Category | Path Finding
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.bfs(startNode [, relTypes, direction, maxDepth])
YIELD node, depth
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex from which the traversal starts
| `relTypes` | String | No | all types | Comma-separated relationship types to follow
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `maxDepth` | Integer | No | unbounded | Maximum BFS depth (hop count) from the start node
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity (the start node is not included)
| `depth` | Integer | BFS depth from the start node
|===

*Description*

Performs a Breadth-First Search traversal from a given start node, visiting all reachable vertices in non-decreasing distance order. Each reachable vertex (excluding the start node itself) is yielded together with its BFS depth. An optional `maxDepth` bound limits the traversal radius.

*Use Cases*

- Shortest hop-count paths from a source
- Neighbourhood exploration at a given radius
- Level-order processing of tree or DAG structures

*Example*

[source,cypher]
----
MATCH (start:Person {name:'Alice'})
CALL algo.bfs(start, 'KNOWS', 'BOTH', 3)
YIELD node, depth
RETURN node.name, depth ORDER BY depth
----

*References*

- https://en.wikipedia.org/wiki/Breadth-first_search[Breadth-first search – Wikipedia]

'''

[[algo-dfs]]
==== algo.dfs

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.dfs`
| Category | Path Finding
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.dfs(startNode [, relTypes, direction, maxDepth])
YIELD node, depth
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex from which the traversal starts
| `relTypes` | String | No | all types | Comma-separated relationship types to follow
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `maxDepth` | Integer | No | unbounded | Maximum DFS depth from the start node
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity (the start node is not included)
| `depth` | Integer | DFS discovery depth from the start node
|===

*Description*

Performs an iterative (non-recursive) Depth-First Search from a given start node using an explicit stack, avoiding Java stack-overflow on large graphs. Vertices are yielded in DFS discovery order together with their depth. An optional `maxDepth` bound limits the traversal.

*Use Cases*

- Reachability analysis
- Dependency resolution and cycle detection
- Tree/DAG structure enumeration

*Example*

[source,cypher]
----
MATCH (start:Module {name:'core'})
CALL algo.dfs(start, 'DEPENDS_ON', 'OUT')
YIELD node, depth
RETURN node.name, depth
----

*References*

- https://en.wikipedia.org/wiki/Depth-first_search[Depth-first search – Wikipedia]

'''

[[algo-dijkstra-single-source]]
==== algo.dijkstra.singleSource

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.dijkstra.singleSource`
| Category | Path Finding
| Complexity | CPU
| Min Args | 3
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.dijkstra.singleSource(startNode, relTypes, weightProperty [, direction])
YIELD node, cost
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Source vertex
| `relTypes` | String | Yes | — | Comma-separated relationship types; pass `''` for all types
| `weightProperty` | String | Yes | — | Edge property name to use as weight (must be numeric, non-negative)
| `direction` | String | No | `"OUT"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Reachable vertex identity (the start node is not included)
| `cost` | Double | Minimum total weight from the start node to this vertex
|===

*Description*

Computes the Single-Source Shortest Path (SSSP) from a given start node to every reachable vertex using Dijkstra's algorithm with a binary min-heap.
This procedure extends `<<algo-dijkstra,algo.dijkstra>>` to return results for all reachable targets at once instead of just a single source-target pair.
All edge weights must be non-negative; edges with negative weights are silently ignored.

*Use Cases*

- Distance maps from a hub in transportation networks
- Risk propagation from an origin node
- Identifying the closest facility to every location in a network

*Example*

[source,cypher]
----
MATCH (hub:City {name:'London'})
CALL algo.dijkstra.singleSource(hub, 'ROAD', 'distance', 'OUT')
YIELD node, cost
RETURN node.name, cost ORDER BY cost LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm[Dijkstra's algorithm – Wikipedia]

See also: <<algo-dijkstra>> for the single source-target variant.

'''

[[algo-longest-path-dag]]
==== algo.longestPath

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.longestPath`
| Category | Path Finding
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.longestPath([relTypes, weightProperty])
YIELD node, distance, source
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types to follow
| `weightProperty` | String | No | `1.0` per hop | Edge property name to use as weight
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `distance` | Double | Maximum distance from the furthest source reaching this vertex
| `source` | RID | The source vertex that achieves the maximum distance
|===

*Description*

Finds the longest path in a Directed Acyclic Graph (DAG) using dynamic programming on topological order (Kahn's algorithm). If the graph contains a cycle the procedure detects it and returns an empty result set.
When no `weightProperty` is provided, each hop has weight `1.0` (i.e., the result is the maximum hop count).

*Use Cases*

- Critical path analysis in project scheduling (CPM)
- Longest dependency chain in build systems
- Pipeline bottleneck identification

*Example*

[source,cypher]
----
CALL algo.longestPath('DEPENDS_ON')
YIELD node, distance, source
RETURN node.name AS task, distance, source.name AS criticalSource
ORDER BY distance DESC LIMIT 5
----

*References*

- https://en.wikipedia.org/wiki/Longest_path_problem[Longest path problem – Wikipedia]

See also: <<algo-topological-sort>> for topological ordering.

'''

[[algo-msa]]
==== algo.msa

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.msa`
| Category | Path Finding
| Complexity | CPU
| Min Args | 1
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.msa(rootNode [, relTypes, weightProperty]) YIELD source, target, weight, totalWeight
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `rootNode` | Vertex | — | Root vertex for the arborescence
| `relTypes` | String | all types | Comma-separated edge type names to traverse
| `weightProperty` | String | `null` | Edge property for weights; `null` = all weights 1.0
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `source` | Vertex | Source vertex of this MSA edge
| `target` | Vertex | Destination vertex of this MSA edge
| `weight` | Float | Weight of this edge
| `totalWeight` | Float | Total weight of the arborescence (same on every row)
|===

*Description*

Computes the Minimum Spanning Arborescence (directed MST) rooted at a specified vertex using the Chu-Liu/Edmonds algorithm. Unlike the undirected `algo.mst`, the arborescence is a *directed* spanning tree where every non-root vertex has exactly one incoming edge and all vertices are reachable from the root along directed paths.

Returns n − 1 rows (one per arborescence edge). Returns an empty result if no spanning arborescence exists (the directed graph is not strongly connected from the root).

*Use Cases*

- Optimising directed distribution networks (water, power, logistics)
- Dependency resolution with minimum total cost
- Directed network backbone extraction

*Example*

[source,cypher]
----
MATCH (r:City {name:'Rome'})
CALL algo.msa(r, 'ROAD', 'distance')
YIELD source, target, weight, totalWeight
RETURN source.name, target.name, weight ORDER BY weight
----

*References*

- https://en.wikipedia.org/wiki/Edmonds%27_algorithm[Edmonds' algorithm – Wikipedia]
- Chu, Y. J. & Liu, T. H. (1965). On the Shortest Arborescence of a Directed Graph. _Science Sinica_.

'''

[[algo-steiner-tree]]
==== algo.steinerTree

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.steinerTree`
| Category | Path Finding
| Complexity | CPU+RAM
| Min Args | 1
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.steinerTree(terminalNodes [, relTypes, weightProperty]) YIELD source, target, weight, totalWeight
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `terminalNodes` | List<Vertex> | — | List of terminal (required) vertices to connect
| `relTypes` | String | all types | Comma-separated edge type names to traverse (undirected)
| `weightProperty` | String | `null` | Edge property for weights; `null` = all weights 1.0
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `source` | Vertex | Source vertex of this Steiner-tree edge
| `target` | Vertex | Destination vertex of this Steiner-tree edge
| `weight` | Float | Weight of this edge
| `totalWeight` | Float | Total Steiner-tree weight (same on every row)
|===

*Description*

Finds an approximate minimum-weight connected subgraph that spans all terminal vertices, using the Kou-Markowsky-Berman 2-approximation algorithm (approximation ratio `2(1 − 1/|T|)` where |T| is the number of terminals):

1. Run Dijkstra from each terminal to find shortest paths to all other terminals.
2. Build a complete graph on terminals with shortest-path distances.
3. Find the MST of this terminal graph (Kruskal's).
4. Expand each MST edge back to its actual shortest path in the original graph.
5. Prune non-terminal leaves iteratively.

Edges are traversed undirected. Returns an empty result if terminals are unreachable from each other.

*Use Cases*

- Designing minimum-cost subnetworks connecting mandatory waypoints
- Content distribution network planning
- Pipeline / cable routing connecting required endpoints

*Example*

[source,cypher]
----
MATCH (a:City {name:'Rome'}), (b:City {name:'Milan'}), (c:City {name:'Naples'})
CALL algo.steinerTree([a, b, c], 'ROAD', 'distance')
YIELD source, target, weight, totalWeight
RETURN source.name, target.name, weight
----

*References*

- https://en.wikipedia.org/wiki/Steiner_tree_problem[Steiner tree problem – Wikipedia]
- Kou, L., Markowsky, G. & Berman, L. (1981). A fast algorithm for Steiner trees. _Acta Informatica_.

'''

=== Centrality

[[algo-page-rank]]
==== algo.pagerank

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.pagerank`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.pagerank([{dampingFactor: 0.85, maxIterations: 20, tolerance: 0.0001, weightProperty: null}])
YIELD node, score
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| config map | Map | No | see defaults | Configuration properties (all optional)
| `dampingFactor` | Double | No | `0.85` | Probability of following an edge vs. teleporting
| `maxIterations` | Integer | No | `20` | Maximum number of power-iteration steps
| `tolerance` | Double | No | `0.0001` | Convergence threshold (max score change)
| `weightProperty` | String | No | `null` | Edge property for weighted PageRank
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `score` | Double | PageRank score
|===

*Description*

Computes the PageRank centrality score for every vertex using the power-iteration method. Dangling nodes (no outgoing edges) contribute their rank to all nodes proportionally via the teleportation mechanism. When `weightProperty` is specified, edge weights are used as transition probabilities (normalized per node).

*Use Cases*

- Web page and document importance ranking
- Identifying authoritative nodes in citation networks
- Social network influence scoring

*Example*

[source,cypher]
----
CALL algo.pagerank({dampingFactor: 0.85, maxIterations: 30})
YIELD node, score
RETURN node, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/PageRank[PageRank – Wikipedia]

'''

[[algo-betweenness]]
==== algo.betweenness

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.betweenness`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.betweenness([{normalized: true}])
YIELD node, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| config map | Map | No | see defaults | Configuration properties (all optional)
| `normalized` | Boolean | No | `true` | Divide scores by `2/((n-1)(n-2))` to normalize to [0,1]
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `score` | Double | Betweenness centrality score
|===

*Description*

Measures how often a vertex lies on the shortest path between all other pairs, using the Brandes algorithm with stack-based back-propagation. When `normalized` is `true`, scores are divided by `2/((n-1)(n-2))` (undirected normalization factor).

*Use Cases*

- Identifying bridge nodes and bottlenecks in networks
- Communication hub detection in social graphs
- Critical infrastructure analysis

*Example*

[source,cypher]
----
CALL algo.betweenness({normalized: true})
YIELD node, score
RETURN node, score ORDER BY score DESC LIMIT 20
----

*References*

- https://en.wikipedia.org/wiki/Betweenness_centrality[Betweenness centrality – Wikipedia]
- Brandes, U. (2001). A faster algorithm for betweenness centrality. _Journal of Mathematical Sociology_, 25(2), 163–177.

'''

[[algo-closeness]]
==== algo.closeness

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.closeness`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.closeness([relTypes, direction, normalized])
YIELD node, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `normalized` | Boolean | No | `true` | Use Wasserman-Faust formula for disconnected graphs
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `score` | Double | Closeness centrality score
|===

*Description*

Computes closeness centrality via BFS from each vertex. For disconnected graphs, the Wasserman-Faust formula is applied: `score = (reachable / sumDist) × (reachable / (n-1))`, where `reachable` is the number of vertices reached and `sumDist` is the sum of shortest path distances to those vertices.

*Use Cases*

- Identifying well-positioned nodes for information spread
- Supply chain hub identification
- Social network influence reach analysis

*Example*

[source,cypher]
----
CALL algo.closeness('KNOWS', 'BOTH', true)
YIELD node, score
RETURN node, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Closeness_centrality[Closeness centrality – Wikipedia]
- Wasserman, S. & Faust, K. (1994). _Social Network Analysis_.

'''

[[algo-degree]]
==== algo.degree

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.degree`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.degree([relTypes, direction])
YIELD node, inDegree, outDegree, degree, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `inDegree` | Long | Number of incoming edges
| `outDegree` | Long | Number of outgoing edges
| `degree` | Long | Total degree (in + out for BOTH, else directional)
| `score` | Double | Normalized degree: `total / (n-1)`
|===

*Description*

Efficiently counts in-degree, out-degree, and total degree for every vertex using `vertex.countEdges()` to avoid materializing edge objects. The normalized score divides total degree by `(n-1)`, representing the fraction of all possible connections.

*Use Cases*

- Identifying high-degree hubs in social networks
- Popularity ranking by connection count
- Network connectivity baseline measurement

*Example*

[source,cypher]
----
CALL algo.degree('FOLLOWS', 'IN')
YIELD node, inDegree, score
RETURN node, inDegree, score ORDER BY inDegree DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Degree_(graph_theory)[Degree (graph theory) – Wikipedia]

'''

[[algo-harmonic]]
==== algo.harmonic

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.harmonic`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.harmonic([relTypes, direction, normalized])
YIELD node, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `normalized` | Boolean | No | `true` | Divide raw score by `(n-1)`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `score` | Double | Harmonic centrality score
|===

*Description*

Computes harmonic centrality as the sum of reciprocal shortest-path distances from a vertex to all reachable vertices: `score = Σ 1/dist(v, u)`. When `normalized` is `true`, divides by `(n-1)`. Harmonic centrality handles disconnected graphs naturally (unreachable vertices contribute 0 to the sum), unlike closeness centrality.

*Use Cases*

- Centrality measure robust to disconnected components
- Influence reach in sparse networks
- Alternative to closeness in disconnected graphs

*Example*

[source,cypher]
----
CALL algo.harmonic('KNOWS', 'BOTH', true)
YIELD node, score
RETURN node, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Closeness_centrality#Harmonic_centrality[Harmonic centrality – Wikipedia]

'''

[[algo-eigenvector]]
==== algo.eigenvector

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.eigenvector`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.eigenvector([relTypes, direction, maxIterations, tolerance])
YIELD node, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `maxIterations` | Integer | No | `20` | Maximum power-iteration steps
| `tolerance` | Double | No | `1e-6` | Convergence threshold
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `score` | Double | Eigenvector centrality score in [0, 1]
|===

*Description*

Computes eigenvector centrality using the power iteration method. A node scores highly if it is connected to other high-scoring nodes. L∞ normalization (divide by maximum score) keeps values in [0, 1]. Convergence is checked against the maximum absolute change in scores between iterations.

*Use Cases*

- Identifying globally influential nodes
- Academic citation influence analysis
- Prestige scoring in directed networks

*Example*

[source,cypher]
----
CALL algo.eigenvector('CITES', 'IN', 50, 1e-8)
YIELD node, score
RETURN node, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Eigenvector_centrality[Eigenvector centrality – Wikipedia]

'''

[[algo-hits]]
==== algo.hits

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.hits`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.hits([relTypes, maxIterations, tolerance])
YIELD node, hubScore, authorityScore
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `maxIterations` | Integer | No | `20` | Maximum number of iterations
| `tolerance` | Double | No | `1e-6` | Convergence threshold (max score change)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `hubScore` | Double | Hub score (good hub points to good authorities)
| `authorityScore` | Double | Authority score (good authority is pointed to by good hubs)
|===

*Description*

Computes Hyperlink-Induced Topic Search (HITS) scores using alternating updates: authority scores are the sum of hub scores of incoming neighbors; hub scores are the sum of authority scores of outgoing neighbors. Both score vectors are L2-normalized after each iteration.

*Use Cases*

- Web link analysis (hubs = index pages, authorities = content pages)
- Identifying experts vs. curators in knowledge networks
- Bipartite influence analysis

*Example*

[source,cypher]
----
CALL algo.hits('LINKS_TO', 30, 1e-8)
YIELD node, hubScore, authorityScore
RETURN node, hubScore, authorityScore ORDER BY authorityScore DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/HITS_algorithm[HITS algorithm – Wikipedia]
- Kleinberg, J. M. (1999). Authoritative sources in a hyperlinked environment. _Journal of the ACM_, 46(5), 604–632.

'''

[[algo-katz]]
==== algo.katz

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.katz`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.katz([relTypes, alpha, maxIterations, tolerance])
YIELD nodeId, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `alpha` | Double | No | `0.005` | Attenuation factor (must be less than 1/λ_max)
| `maxIterations` | Integer | No | `100` | Maximum number of iterations
| `tolerance` | Double | No | `1e-6` | Convergence threshold
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity
| `score` | Double | Katz centrality score (normalized to [0, 1])
|===

*Description*

Computes Katz centrality, which counts all paths between nodes with exponential decay by path length. Formula: `k[i] = alpha × Σ_j(A[j][i] × k[j]) + 1`, normalized by the maximum score. Unlike eigenvector centrality, every node receives a base score (the `+1` term), making it robust for directed and sparse graphs.

*Use Cases*

- Influence ranking in directed social networks
- Ranking nodes when direct connection is sparse
- Early-stage recommendation systems

*Example*

[source,cypher]
----
CALL algo.katz('FOLLOWS', 0.01, 50, 1e-8)
YIELD nodeId, score
RETURN nodeId, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Katz_centrality[Katz centrality – Wikipedia]

'''

[[algo-vote-rank]]
==== algo.voteRank

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.voteRank`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.voteRank([relTypes, topK])
YIELD nodeId, rank
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `topK` | Integer | No | all nodes | Maximum number of top spreaders to return
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity
| `rank` | Integer | 1-based rank (1 = most influential spreader)
|===

*Description*

Identifies the most influential spreaders in a network using an iterative election process. In each round, the node with the highest accumulated votes is selected as a spreader. After selection, each of its neighbors has its voting ability reduced by `1/degree`. This suppression mechanism ensures that the selected spreaders are structurally diverse rather than clustered together.

*Use Cases*

- Viral marketing seed set selection
- Identifying structurally independent influencers
- Epidemic intervention: selecting individuals for immunization

*Example*

[source,cypher]
----
CALL algo.voteRank('KNOWS', 5)
YIELD nodeId, rank
RETURN nodeId, rank ORDER BY rank ASC
----

*References*

- Zhang, J., et al. (2016). Identifying a set of influential spreaders in complex networks. _Scientific Reports_, 6, 27823.

'''

[[algo-eccentricity]]
==== algo.eccentricity

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.eccentricity`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.eccentricity([relTypes, direction])
YIELD node, eccentricity, isCenter, isPeripheral
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `eccentricity` | Integer | Maximum shortest-path distance to any reachable vertex
| `isCenter` | Boolean | `true` if eccentricity equals the graph radius
| `isPeripheral` | Boolean | `true` if eccentricity equals the graph diameter
|===

*Description*

Computes the eccentricity of each vertex (the greatest shortest-path distance to any other reachable vertex) via BFS from each node. The graph radius is the minimum eccentricity and the graph diameter is the maximum. Center nodes have eccentricity equal to the radius; peripheral nodes have eccentricity equal to the diameter.

*Use Cases*

- Graph center identification for optimal facility placement
- Network diameter and radius computation
- Peripheral node analysis for vulnerability assessment

*Example*

[source,cypher]
----
CALL algo.eccentricity('ROAD', 'BOTH')
YIELD node, eccentricity, isCenter, isPeripheral
RETURN node, eccentricity, isCenter, isPeripheral ORDER BY eccentricity ASC
----

*References*

- https://en.wikipedia.org/wiki/Distance_(graph_theory)[Distance (graph theory) – Wikipedia]

'''

[[algo-personalized-page-rank]]
==== algo.personalizedPageRank

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.personalizedPageRank`
| Category | Centrality
| Complexity | CPU
| Min Args | 1
| Max Args | 5
|===

*Syntax*

[source,cypher]
----
CALL algo.personalizedPageRank(sourceNode [, relTypes, dampingFactor, maxIterations, tolerance])
YIELD nodeId, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `sourceNode` | Vertex | Yes | — | The personalization origin vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `dampingFactor` | Double | No | `0.85` | Probability of following an edge
| `maxIterations` | Integer | No | `20` | Maximum power-iteration steps
| `tolerance` | Double | No | `1e-6` | Convergence threshold
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity
| `score` | Double | Personalized PageRank score relative to the source node
|===

*Description*

Computes Personalized PageRank (PPR) relative to a single source vertex. Unlike standard PageRank, the teleportation probability is concentrated entirely at the source node (personalization vector = 1 at source, 0 everywhere else). Scores represent structural proximity/importance relative to the source. Dangling nodes contribute their rank back to the source only.

*Use Cases*

- Node-centric similarity and proximity ranking
- Recommendation systems ("nodes similar to X")
- Query-centric graph exploration

*Example*

[source,cypher]
----
MATCH (s:Person {name:'Alice'})
CALL algo.personalizedPageRank(s, 'KNOWS', 0.85, 20, 0.000001)
YIELD nodeId, score
RETURN nodeId, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/PageRank#Personalized_PageRank[Personalized PageRank – Wikipedia]

'''

[[algo-article-rank]]
==== algo.articlerank

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.articlerank`
| Category | Centrality
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.articlerank([config]) YIELD node, score
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `dampingFactor` | Float | `0.85` | Damping factor (probability of following a link)
| `maxIterations` | Integer | `20` | Maximum number of iterations
| `tolerance` | Float | `0.0001` | Convergence threshold (sum of absolute score changes)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `score` | Float | ArticleRank score for the vertex
|===

*Description*

ArticleRank is a variant of PageRank that dampens the contribution of low-degree nodes. While standard PageRank divides a node's rank equally among its out-degree neighbours, ArticleRank divides by `outDegree(v) + avgOutDegree`. This reduces the influence of nodes with very few connections, producing more balanced centrality scores in sparse or scale-free graphs.

Formula: `AR(u) = (1 − d) / N + d × Σ AR(v) / (outDeg(v) + avgOutDeg)`

*Use Cases*

- Web page importance ranking where low-citation pages should have less influence
- Academic citation networks with highly skewed degree distributions
- Any domain where hub dampening is desirable

*Example*

[source,cypher]
----
CALL algo.articlerank({dampingFactor: 0.85, maxIterations: 20})
YIELD node, score
RETURN node.name AS name, score
ORDER BY score DESC LIMIT 10
----

*References*

- https://dl.acm.org/doi/10.1145/988672.988727[ArticleRank: a PageRank-based alternative to numbers of citations – WWW 2004]

'''

=== Community Detection

[[algo-wcc]]
==== algo.wcc

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.wcc`
| Category | Community Detection
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.wcc([relTypes])
YIELD node, componentId
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `componentId` | Integer | Sequential component identifier (0-based)
|===

*Description*

Identifies weakly connected components using BFS, treating all edges as undirected regardless of their actual direction. Two vertices are in the same component if there exists any undirected path between them. Component IDs are remapped to sequential integers starting at 0.

*Use Cases*

- Data quality: detecting isolated subgraphs
- Network partition detection
- Graph reachability analysis

*Example*

[source,cypher]
----
CALL algo.wcc('CONNECTED_TO')
YIELD node, componentId
RETURN componentId, count(node) AS size ORDER BY size DESC
----

*References*

- https://en.wikipedia.org/wiki/Component_(graph_theory)[Connected component – Wikipedia]

'''

[[algo-scc]]
==== algo.scc

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.scc`
| Category | Community Detection
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.scc([relTypes])
YIELD node, componentId
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `componentId` | Integer | Sequential component identifier (0-based)
|===

*Description*

Identifies strongly connected components using Kosaraju's two-pass algorithm: iterative DFS on the original graph to compute finish-time ordering, then BFS on the reversed graph in reverse finish order. A strongly connected component is a maximal set of vertices where every vertex is reachable from every other.

*Use Cases*

- Deadlock detection in dependency graphs
- Circular dependency analysis in software systems
- Identifying mutually-reachable clusters in directed networks

*Example*

[source,cypher]
----
CALL algo.scc('DEPENDS_ON')
YIELD node, componentId
RETURN componentId, collect(node) AS members ORDER BY size(members) DESC
----

*References*

- https://en.wikipedia.org/wiki/Kosaraju%27s_algorithm[Kosaraju's algorithm – Wikipedia]

'''

[[algo-louvain]]
==== algo.louvain

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.louvain`
| Category | Community Detection
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.louvain([{maxIterations: 10, tolerance: 0.0001, weightProperty: null}])
YIELD node, communityId, modularity
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| config map | Map | No | see defaults | Configuration properties (all optional)
| `maxIterations` | Integer | No | `10` | Maximum number of phase-1 passes
| `tolerance` | Double | No | `0.0001` | Stop when modularity gain falls below this value
| `weightProperty` | String | No | `null` | Edge property for weighted modularity
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `communityId` | Integer | Sequential community identifier (0-based)
| `modularity` | Double | Final modularity score of the partition
|===

*Description*

Detects communities by maximizing modularity using the Louvain method: a greedy phase where each node is moved to the neighboring community that maximally increases modularity, iterated until no improvement is found. Community IDs are remapped to sequential 0-based integers.

*Use Cases*

- Social community discovery
- Topic clustering in citation networks
- Customer segmentation in purchase graphs

*Example*

[source,cypher]
----
CALL algo.louvain({maxIterations: 15, weightProperty: 'weight'})
YIELD node, communityId, modularity
RETURN communityId, collect(node) AS members, modularity ORDER BY size(members) DESC
----

*References*

- https://en.wikipedia.org/wiki/Louvain_method[Louvain method – Wikipedia]

'''

[[algo-leiden]]
==== algo.leiden

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.leiden`
| Category | Community Detection
| Complexity | CPU
| Min Args | 0
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.leiden([relTypes, maxIterations, resolution])
YIELD nodeId, community
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `maxIterations` | Integer | No | `10` | Maximum number of local-move + refinement iterations
| `resolution` | Double | No | `1.0` | Resolution parameter γ (higher = more, smaller communities)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity
| `community` | Integer | Sequential community identifier (0-based)
|===

*Description*

An improved community detection algorithm over Louvain, featuring a two-phase approach: (1) local greedy moves maximizing modularity, followed by (2) a refinement phase that checks whether nodes should be split out from their community to ensure well-connectedness. The resolution parameter γ controls granularity of the resulting communities.

*Use Cases*

- High-quality community detection with guaranteed well-connectedness
- Hierarchical community structure analysis
- Fine-grained cluster control via resolution parameter

*Example*

[source,cypher]
----
CALL algo.leiden('KNOWS', 10, 1.0)
YIELD nodeId, community
RETURN community, count(nodeId) AS size ORDER BY size DESC
----

*References*

- Traag, V. A., Waltman, L., & van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing well-connected communities. _Scientific Reports_, 9, 5233.

'''

[[algo-label-propagation]]
==== algo.labelpropagation

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.labelpropagation`
| Category | Community Detection
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.labelpropagation([{maxIterations: 10, direction: 'BOTH'}])
YIELD node, communityId
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| config map | Map | No | see defaults | Configuration properties (all optional)
| `maxIterations` | Integer | No | `10` | Maximum number of propagation passes
| `direction` | String | No | `"BOTH"` | Edge direction to follow: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `communityId` | Integer | Community label assigned to this vertex
|===

*Description*

Detects communities by iteratively propagating labels. Each vertex adopts the most frequent label among its neighbors; ties are broken by choosing the smallest label. Vertex processing order is randomly shuffled each iteration. Converges quickly in practice but may produce non-deterministic results.

*Use Cases*

- Fast community detection in very large graphs
- Semi-supervised classification when some labels are pre-assigned
- Near-linear time community discovery

*Example*

[source,cypher]
----
CALL algo.labelpropagation({maxIterations: 20, direction: 'OUT'})
YIELD node, communityId
RETURN communityId, count(node) AS size ORDER BY size DESC
----

*References*

- https://en.wikipedia.org/wiki/Label_propagation_algorithm[Label propagation algorithm – Wikipedia]

'''

[[algo-hierarchical-clustering]]
==== algo.hierarchicalClustering

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.hierarchicalClustering`
| Category | Community Detection
| Complexity | CPU+RAM
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.hierarchicalClustering([relTypes, numClusters])
YIELD nodeId, cluster
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `numClusters` | Integer | No | `2` | Desired number of clusters
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity
| `cluster` | Integer | Assigned cluster identifier
|===

*Description*

Performs agglomerative (bottom-up) hierarchical clustering using single-linkage: each vertex starts in its own cluster, and the two clusters with the highest Jaccard similarity between their neighbor sets are merged iteratively until the target number of clusters is reached. Uses Union-Find for efficient cluster management.

*Use Cases*

- Dendrogram-style cluster analysis
- Grouping vertices by structural neighborhood similarity
- Coarse-grained community assignment with controlled count

*Example*

[source,cypher]
----
CALL algo.hierarchicalClustering('SIMILAR_TO', 5)
YIELD nodeId, cluster
RETURN cluster, count(nodeId) AS size ORDER BY size DESC
----

*References*

- https://en.wikipedia.org/wiki/Hierarchical_clustering[Hierarchical clustering – Wikipedia]

'''

[[algo-slpa]]
==== algo.slpa

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.slpa`
| Category | Community Detection
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.slpa([config]) YIELD node, communities
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `iterations` | Integer | `20` | Number of propagation iterations (T)
| `threshold` | Float | `0.1` | Minimum label frequency in memory to be included in output
| `seed` | Long | `-1` | Random seed for reproducibility; `-1` uses a random seed
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `communities` | List<Long> | List of community IDs the vertex belongs to (overlapping)
|===

*Description*

SLPA (Speaker-Listener Label Propagation Algorithm) detects *overlapping* communities. Each node maintains a memory of labels it has received over T iterations. In each iteration every node acts as a speaker (emitting its most frequent memory label) and a listener (recording the most common label from its neighbours). After T rounds, labels appearing with frequency ≥ threshold are retained as community assignments.

Unlike non-overlapping algorithms (Louvain, Label Propagation), SLPA allows each vertex to belong to multiple communities simultaneously, making it suitable for social networks where individuals naturally participate in multiple groups.

*Use Cases*

- Overlapping community detection in social networks
- Finding cross-cutting roles in collaboration networks
- Identifying nodes that bridge multiple communities

*Example*

[source,cypher]
----
CALL algo.slpa({iterations: 20, threshold: 0.1, seed: 42})
YIELD node, communities
RETURN node.name AS name, communities
ORDER BY size(communities) DESC
----

*References*

- https://doi.org/10.1109/IPDPS.2012.111[Xie, J. et al. (2013). SLPA: Uncovering Overlapping Communities in Social Networks via a Speaker-listener Interaction Dynamic Process. _ICDMW 2011_]

'''

[[algo-max-k-cut]]
==== algo.maxKCut

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.maxKCut`
| Category | Community Detection
| Complexity | CPU
| Min Args | 1
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.maxKCut(k [, config]) YIELD node, community, cutWeight
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `k` | Integer | — | Number of partitions (≥ 2)
| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `maxIterations` | Integer | `100` | Maximum local-search passes per restart
| `restarts` | Integer | `3` | Number of random restarts (best result returned)
| `weightProperty` | String | `null` | Edge property for weights; `null` = all weights 1.0
| `relTypes` | String | all types | Comma-separated edge type names
| `direction` | String | `BOTH` | Edge traversal direction: `IN`, `OUT`, or `BOTH`
| `seed` | Long | `-1` | Random seed; -1 = random
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `community` | Integer | Partition index in range [0, k − 1]
| `cutWeight` | Float | Total weight of inter-partition edges (same on every row)
|===

*Description*

Approximates the Maximum k-Cut problem: partition all vertices into k disjoint subsets to maximise the total weight of edges that cross between different partitions. Uses a greedy local-search heuristic:

1. Randomly assign each vertex to one of k partitions.
2. For each vertex, compute the cut gain for every possible partition and move it to the best one.
3. Repeat until no vertex can be moved to improve the cut (or `maxIterations` is reached).
4. Repeat steps 1–3 for `restarts` independent runs and return the best result found.

This heuristic achieves a `(k−1)/k` fraction of the optimal cut on unweighted graphs and converges quickly in practice.

*Use Cases*

- Graph partitioning for parallel processing
- Community detection via balanced cut
- Circuit bipartitioning and VLSI placement

*Example*

[source,cypher]
----
CALL algo.maxKCut(3, {restarts: 5, seed: 42})
YIELD node, community, cutWeight
RETURN node.name AS name, community
ORDER BY community
----

*References*

- https://en.wikipedia.org/wiki/Maximum_cut[Maximum cut – Wikipedia]
- Sahni, S. & Gonzalez, T. (1976). P-Complete Approximation Problems. _Journal of the ACM_.

'''

=== Structural Analysis

[[algo-k-core]]
==== algo.kcore

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.kcore`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.kcore([relTypes])
YIELD node, coreNumber
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `coreNumber` | Integer | Maximum k for which this vertex belongs to a k-core
|===

*Description*

Computes the k-core decomposition of the graph using the Batagelj-Zaversnik O(V+E) algorithm with bucket sort. The k-core of a graph is the maximal subgraph in which every vertex has at least degree k. Each vertex is assigned its coreness: the maximum k for which it belongs to a k-core.

*Use Cases*

- Identifying dense graph cores and peripheral nodes
- Network robustness analysis
- Cohesive subgroup detection

*Example*

[source,cypher]
----
CALL algo.kcore('KNOWS')
YIELD node, coreNumber
RETURN node, coreNumber ORDER BY coreNumber DESC LIMIT 20
----

*References*

- Batagelj, V. & Zaversnik, M. (2003). An O(m) Algorithm for Cores Decomposition of Networks. _arXiv:cs/0310049_.

'''

[[algo-triangle-count]]
==== algo.triangleCount

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.triangleCount`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.triangleCount([relTypes])
YIELD node, triangles, clusteringCoefficient
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `triangles` | Integer | Number of triangles the vertex participates in
| `clusteringCoefficient` | Double | Local clustering coefficient
|===

*Description*

Counts the number of closed triangles for each vertex using BitSet intersection of neighbor sets (minimal GC pressure). The local clustering coefficient is computed as `2 × triangles / (deg × (deg-1))`, measuring how close the vertex's neighbors are to forming a complete subgraph.

*Use Cases*

- Social network cohesiveness analysis
- Spam detection (low clustering = suspicious accounts)
- Community structure pre-screening

*Example*

[source,cypher]
----
CALL algo.triangleCount('KNOWS')
YIELD node, triangles, clusteringCoefficient
RETURN node, triangles, clusteringCoefficient ORDER BY triangles DESC LIMIT 20
----

*References*

- https://en.wikipedia.org/wiki/Clustering_coefficient[Clustering coefficient – Wikipedia]

'''

[[algo-articulation-points]]
==== algo.articulationPoints

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.articulationPoints`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.articulationPoints([relTypes])
YIELD node
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity of an articulation point
|===

*Description*

Finds all articulation points (cut vertices) — vertices whose removal would increase the number of connected components — using Tarjan's iterative DFS algorithm with discovery time and low-link arrays. Only vertices identified as articulation points are returned; all other vertices produce no result rows.

*Use Cases*

- Network resilience analysis: single points of failure
- Critical infrastructure node identification
- Dependency chain vulnerability assessment

*Example*

[source,cypher]
----
CALL algo.articulationPoints('CONNECTED_TO')
YIELD node
RETURN node
----

*References*

- https://en.wikipedia.org/wiki/Biconnected_component[Biconnected component – Wikipedia]

'''

[[algo-bridges]]
==== algo.bridges

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.bridges`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.bridges([relTypes])
YIELD source, target
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `source` | RID | Source vertex of the bridge edge
| `target` | RID | Target vertex of the bridge edge
|===

*Description*

Finds all bridge edges — edges whose removal would disconnect the graph — using Tarjan's iterative DFS. A bridge is detected when the low-link value of the child strictly exceeds the discovery time of the parent: `low[v] > disc[parent]`. Uses OUT-direction adjacency for DFS traversal.

*Use Cases*

- Network resilience: critical links whose failure causes disconnection
- Infrastructure planning: bottleneck link identification
- Supply chain single-path dependency detection

*Example*

[source,cypher]
----
CALL algo.bridges('LINK')
YIELD source, target
RETURN source, target
----

*References*

- https://en.wikipedia.org/wiki/Bridge_(graph_theory)[Bridge (graph theory) – Wikipedia]

'''

[[algo-mst]]
==== algo.mst

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.mst`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.mst([weightProperty, relTypes])
YIELD source, target, weight, totalWeight
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `weightProperty` | String | No | uniform weight | Edge property name for weights
| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `source` | RID | Source vertex of the MST edge
| `target` | RID | Target vertex of the MST edge
| `weight` | Double | Weight of this MST edge
| `totalWeight` | Double | Total weight of the entire MST (same on every row)
|===

*Description*

Computes the minimum spanning tree of the graph using Kruskal's algorithm with a Union-Find data structure (path halving + union by rank). Edges are sorted by weight; each edge is added if it connects two different components. For disconnected graphs, a minimum spanning forest is returned.

*Use Cases*

- Network infrastructure with minimal cost
- Clustering by minimal connectivity
- Dendrogram construction for hierarchical analysis

*Example*

[source,cypher]
----
CALL algo.mst('cost', 'CABLE')
YIELD source, target, weight, totalWeight
RETURN source, target, weight, totalWeight ORDER BY weight ASC
----

*References*

- https://en.wikipedia.org/wiki/Kruskal%27s_algorithm[Kruskal's algorithm – Wikipedia]

'''

[[algo-topological-sort]]
==== algo.topologicalSort

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.topologicalSort`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.topologicalSort([relTypes])
YIELD node, order
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `order` | Integer | Topological position (0-based); `-1` if the node is part of a cycle
|===

*Description*

Computes a topological ordering of the vertices of a directed acyclic graph (DAG) using Kahn's BFS algorithm. Nodes in cycles cannot be topologically sorted and receive `order = -1`. The algorithm processes nodes with zero in-degree first, progressively reducing in-degrees of successors.

*Use Cases*

- Build system dependency resolution
- Task scheduling with ordering constraints
- Compilation order determination

*Example*

[source,cypher]
----
CALL algo.topologicalSort('DEPENDS_ON')
YIELD node, order
RETURN node, order ORDER BY order ASC
----

*References*

- https://en.wikipedia.org/wiki/Topological_sorting[Topological sorting – Wikipedia]

'''

[[algo-bipartite]]
==== algo.bipartite

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.bipartite`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.bipartite([relTypes])
YIELD node, partition, isBipartite
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `partition` | Integer | Partition assignment: `0` or `1`
| `isBipartite` | Boolean | Global flag: `true` if the graph is bipartite
|===

*Description*

Tests whether the graph is bipartite by attempting 2-coloring via BFS. A graph is bipartite if and only if it contains no odd-length cycles. The algorithm correctly handles disconnected graphs by running BFS from each unvisited component. All rows share the same `isBipartite` value.

*Use Cases*

- Verifying bipartite structure before applying bipartite-specific algorithms
- Recommendation system graph validation (users ↔ items)
- Conflict-free scheduling verification

*Example*

[source,cypher]
----
CALL algo.bipartite('CONNECTS')
YIELD node, partition, isBipartite
RETURN isBipartite, partition, collect(node) AS members
----

*References*

- https://en.wikipedia.org/wiki/Bipartite_graph[Bipartite graph – Wikipedia]

'''

[[algo-graph-coloring]]
==== algo.graphColoring

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.graphColoring`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.graphColoring([relTypes])
YIELD node, color, chromaticNumber
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `color` | Integer | Assigned color (0-based integer)
| `chromaticNumber` | Integer | Global chromatic number of the coloring (same on every row)
|===

*Description*

Assigns colors to vertices such that no two adjacent vertices share the same color, using a greedy algorithm with a `forbidden[]` array to track unavailable colors for each vertex. The result is a valid coloring but not guaranteed to use the minimum number of colors (the chromatic number). The greedy approach works well in practice for most graph types.

*Use Cases*

- Exam/exam-room scheduling (students sharing courses cannot share a room)
- Frequency assignment in radio networks
- Register allocation in compilers

*Example*

[source,cypher]
----
CALL algo.graphColoring('CONFLICTS_WITH')
YIELD node, color, chromaticNumber
RETURN chromaticNumber, color, collect(node) AS group
----

*References*

- https://en.wikipedia.org/wiki/Graph_coloring[Graph coloring – Wikipedia]

'''

[[algo-cycle-detection]]
==== algo.cycleDetection

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.cycleDetection`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.cycleDetection([relTypes])
YIELD node, inCycle, hasCycle
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `inCycle` | Boolean | `true` if the vertex is part of a cycle
| `hasCycle` | Boolean | Global flag: `true` if the graph contains any cycle
|===

*Description*

Detects cycles using Kosaraju's SCC algorithm. A vertex is considered to be in a cycle if it belongs to an SCC of size greater than 1, or if it has a self-loop. The global `hasCycle` flag is `true` if any vertex satisfies this condition.

*Use Cases*

- Dependency graph cycle detection (circular dependencies)
- Deadlock possibility analysis
- DAG validation before topological sort

*Example*

[source,cypher]
----
CALL algo.cycleDetection('DEPENDS_ON')
YIELD node, inCycle, hasCycle
RETURN hasCycle, node, inCycle
----

*References*

- https://en.wikipedia.org/wiki/Cycle_detection[Cycle detection – Wikipedia]

'''

[[algo-densest-subgraph]]
==== algo.densestSubgraph

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.densestSubgraph`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.densestSubgraph([relTypes])
YIELD node, inDenseSubgraph, density
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | RID | Vertex identity
| `inDenseSubgraph` | Boolean | `true` if the vertex is part of the densest subgraph found
| `density` | Double | Edge density of the densest subgraph (same on every row)
|===

*Description*

Finds the densest subgraph using Charikar's greedy peeling 2-approximation algorithm. The algorithm iteratively removes the vertex with the lowest current degree, tracking the density `(edges / nodes)` of the remaining subgraph. The subgraph that achieved the maximum density is returned.

*Use Cases*

- Identifying tightly-knit communities in social networks
- Quasi-clique detection
- Anomaly detection: unusually dense subgraphs

*Example*

[source,cypher]
----
CALL algo.densestSubgraph('KNOWS')
YIELD node, inDenseSubgraph, density
RETURN density, node, inDenseSubgraph ORDER BY inDenseSubgraph DESC
----

*References*

- Charikar, M. (2000). Greedy approximation algorithms for finding dense components in a graph. _APPROX 2000_.

'''

[[algo-clique]]
==== algo.clique

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.clique`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.clique([relTypes, minSize])
YIELD clique, size
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `minSize` | Integer | No | `3` | Minimum clique size to report
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `clique` | List<RID> | List of vertex identities forming the clique
| `size` | Integer | Number of vertices in the clique
|===

*Description*

Enumerates all maximal cliques in the graph using the Bron-Kerbosch algorithm with Tomita pivoting, implemented iteratively with an explicit stack to avoid JVM stack overflow on deep recursion. The graph is treated as undirected (both OUT and IN edges merged). Only cliques of size ≥ `minSize` are returned.

*Use Cases*

- Finding tightly connected groups (friend circles, co-author clusters)
- Network motif analysis
- Dense community detection

*Example*

[source,cypher]
----
CALL algo.clique('KNOWS', 4)
YIELD clique, size
RETURN size, clique ORDER BY size DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Bron%E2%80%93Kerbosch_algorithm[Bron-Kerbosch algorithm – Wikipedia]

'''

[[algo-k-truss]]
==== algo.kTruss

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.kTruss`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.kTruss([relTypes, k])
YIELD nodeId, trussNumber
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `k` | Integer | No | `3` | Target truss parameter (minimum 3)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity
| `trussNumber` | Integer | Maximum k for which this vertex belongs to a k-truss
|===

*Description*

Computes the k-truss decomposition using BitSet-based triangle counting. A k-truss is a maximal subgraph where every edge participates in at least k−2 triangles. The algorithm iteratively removes edges with insufficient triangle support, and each vertex is assigned the maximum truss number of any of its incident edges in the full decomposition.

*Use Cases*

- Dense subgraph discovery beyond k-cores
- Social cohesion analysis (friend of a friend structures)
- Financial fraud ring detection

*Example*

[source,cypher]
----
CALL algo.kTruss('KNOWS', 4)
YIELD nodeId, trussNumber
RETURN nodeId, trussNumber ORDER BY trussNumber DESC LIMIT 20
----

*References*

- Cohen, J. (2008). Trusses: Cohesive subgraphs for social network analysis. _National Security Agency Technical Report_.

'''

[[algo-biconnected-components]]
==== algo.biconnectedComponents

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.biconnectedComponents`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.biconnectedComponents([relTypes]) YIELD node, componentId
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `relTypes` | String | all types | Comma-separated edge type names to traverse
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `componentId` | Integer | Biconnected component identifier
|===

*Description*

Computes biconnected components using an iterative variant of Tarjan's algorithm with an explicit edge stack. A biconnected component is a maximal subgraph with no articulation point — removing any single vertex leaves the component connected. Articulation points (cut vertices) appear in multiple components because they belong to each component on either side of the cut.

The algorithm uses undirected traversal regardless of the edge direction parameter.

*Use Cases*

- Network resilience analysis (identifying single points of failure)
- Circuit board layout and VLSI design
- Social network structural analysis

*Example*

[source,cypher]
----
CALL algo.biconnectedComponents('ROAD')
YIELD node, componentId
RETURN componentId, collect(node.name) AS members
ORDER BY size(members) DESC
----

*References*

- https://en.wikipedia.org/wiki/Biconnected_component[Biconnected component – Wikipedia]
- Tarjan, R. E. (1972). Depth-first search and linear graph algorithms. _SIAM Journal on Computing_.

'''

[[algo-local-clustering-coefficient]]
==== algo.localClusteringCoefficient

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.localClusteringCoefficient`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.localClusteringCoefficient([relTypes]) YIELD node, localClusteringCoefficient
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `relTypes` | String | all types | Comma-separated edge type names to traverse (treated as undirected)
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `localClusteringCoefficient` | Float | LCC value in range [0.0, 1.0]
|===

*Description*

Computes the Local Clustering Coefficient (LCC) for every vertex. The LCC of a node v is the fraction of pairs of neighbours of v that are themselves connected: `LCC(v) = triangles(v) / (k*(k-1)/2)` where k is the degree of v. Nodes with degree < 2 receive a coefficient of 0.0. Triangle counting is performed with a BitSet intersection, making it efficient even for dense graphs.

*Use Cases*

- Identifying tightly-knit communities and cliques
- Detecting community structure (nodes with high LCC are embedded in local clusters)
- Social network cohesion analysis

*Example*

[source,cypher]
----
CALL algo.localClusteringCoefficient('KNOWS')
YIELD node, localClusteringCoefficient
RETURN node.name AS name, localClusteringCoefficient
ORDER BY localClusteringCoefficient DESC LIMIT 20
----

*References*

- https://en.wikipedia.org/wiki/Clustering_coefficient[Clustering coefficient – Wikipedia]
- Watts, D. J. & Strogatz, S. H. (1998). Collective dynamics of 'small-world' networks. _Nature_.

'''

[[algo-bipartite-matching]]
==== algo.bipartiteMatching

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.bipartiteMatching`
| Category | Structural Analysis
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.bipartiteMatching([relTypes]) YIELD node1, node2, matchingSize
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `relTypes` | String | all types | Comma-separated edge type names defining the bipartite graph
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | Vertex | Left-side matched vertex
| `node2` | Vertex | Right-side matched vertex
| `matchingSize` | Integer | Total number of matched pairs (same in every row)
|===

*Description*

Computes a maximum bipartite matching using the Hopcroft-Karp algorithm (O(E√V)). The graph is treated as bipartite: nodes with only outgoing edges are on the left side; nodes with only incoming edges are on the right side. Each output row represents one matched pair. The algorithm finds augmenting paths via BFS (layered graph construction) followed by DFS, maximising the number of matched pairs in O(E√V) time.

Note: The procedure assumes the input graph is already bipartite. Results are undefined for non-bipartite inputs.

*Use Cases*

- Task-worker assignment optimisation
- Resource allocation (jobs to machines)
- Stable marriage / hospital-resident matching
- Recommendation filtering

*Example*

[source,cypher]
----
CALL algo.bipartiteMatching('ASSIGNED_TO')
YIELD node1, node2, matchingSize
RETURN node1.name AS worker, node2.name AS task, matchingSize
----

*References*

- https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm[Hopcroft-Karp algorithm – Wikipedia]
- Hopcroft, J. E. & Karp, R. M. (1973). An n^5/2 algorithm for maximum matchings in bipartite graphs. _SIAM Journal on Computing_.

'''

=== Similarity and Link Prediction

[[algo-jaccard]]
==== algo.jaccard

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.jaccard`
| Category | Similarity
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.jaccard(node [, relTypes, direction, cutoff])
YIELD node1, node2, similarity
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `node` | Vertex | Yes | — | Source vertex to compare against all others
| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `cutoff` | Double | No | `0.0` | Minimum similarity threshold; pairs below are excluded
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | RID | Source vertex identity
| `node2` | RID | Compared vertex identity
| `similarity` | Double | Jaccard similarity: |N(src) ∩ N(v)| / |N(src) ∪ N(v)|
|===

*Description*

Computes the Jaccard similarity between the given source vertex and all other vertices, based on neighborhood overlap. Formula: `similarity = |N(src) ∩ N(v)| / |N(src) ∪ N(v)|`. Pairs with similarity below `cutoff` are filtered out.

*Use Cases*

- Collaborative filtering: "users with similar tastes"
- Friend recommendation based on shared connections
- Document similarity via shared keyword links

*Example*

[source,cypher]
----
MATCH (u:User {name:'Alice'})
CALL algo.jaccard(u, 'LIKES', 'OUT', 0.2)
YIELD node1, node2, similarity
RETURN node2, similarity ORDER BY similarity DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Jaccard_index[Jaccard index – Wikipedia]

'''

[[algo-adamic-adar]]
==== algo.adamicAdar

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.adamicAdar`
| Category | Link Prediction
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.adamicAdar(node [, relTypes, direction, cutoff])
YIELD node1, node2, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `node` | Vertex | Yes | — | Source vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `cutoff` | Double | No | `0.0` | Minimum score threshold
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | RID | Source vertex identity
| `node2` | RID | Compared vertex identity
| `score` | Double | Adamic-Adar score; higher = more likely to form a link
|===

*Description*

Computes the Adamic-Adar link prediction score between the source vertex and all non-adjacent vertices. For each pair, the score is the sum over common neighbors w of `1/log(degree(w))`. Highly-connected common neighbors contribute less than rare common neighbors. Results are sorted by score descending.

*Use Cases*

- Social network friend recommendation
- Knowledge graph link prediction
- Co-authorship prediction in citation networks

*Example*

[source,cypher]
----
MATCH (p:Person {name:'Bob'})
CALL algo.adamicAdar(p, 'KNOWS', 'BOTH', 0.5)
YIELD node1, node2, score
RETURN node2, score ORDER BY score DESC LIMIT 10
----

*References*

- Adamic, L. & Adar, E. (2003). Friends and neighbors on the web. _Social Networks_, 25(3), 211–230.

'''

[[algo-common-neighbors]]
==== algo.commonNeighbors

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.commonNeighbors`
| Category | Link Prediction
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.commonNeighbors(node [, relTypes, direction, cutoff])
YIELD node1, node2, commonNeighbors
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `node` | Vertex | Yes | — | Source vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `cutoff` | Integer | No | `1` | Minimum number of common neighbors
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | RID | Source vertex identity
| `node2` | RID | Compared vertex identity
| `commonNeighbors` | Integer | Count of shared neighbors
|===

*Description*

Counts the number of common neighbors between the source vertex and all other vertices. Pairs with fewer than `cutoff` common neighbors are excluded. Results are sorted by count descending.

*Use Cases*

- Simple friend-of-a-friend recommendation
- Link prediction baseline
- Mutual connection analysis

*Example*

[source,cypher]
----
MATCH (u:User {name:'Charlie'})
CALL algo.commonNeighbors(u, 'FOLLOWS', 'BOTH', 3)
YIELD node1, node2, commonNeighbors
RETURN node2, commonNeighbors ORDER BY commonNeighbors DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Link_prediction[Link prediction – Wikipedia]

'''

[[algo-preferential-attachment]]
==== algo.preferentialAttachment

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.preferentialAttachment`
| Category | Link Prediction
| Complexity | CPU
| Min Args | 1
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.preferentialAttachment(node [, relTypes, direction])
YIELD node1, node2, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `node` | Vertex | Yes | — | Source vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | RID | Source vertex identity
| `node2` | RID | Compared vertex identity
| `score` | Long | Product of the two nodes' degrees
|===

*Description*

Computes the preferential attachment score for each pair as `degree(node1) × degree(node2)`. Based on the observation that in evolving networks, new links preferentially form between high-degree nodes. A simple but effective baseline for link prediction in scale-free networks.

*Use Cases*

- Scale-free network link prediction
- Social network "people you may know" baseline
- Network growth modeling

*Example*

[source,cypher]
----
MATCH (u:User {id:1})
CALL algo.preferentialAttachment(u, 'KNOWS', 'BOTH')
YIELD node1, node2, score
RETURN node2, score ORDER BY score DESC LIMIT 10
----

*References*

- https://en.wikipedia.org/wiki/Preferential_attachment[Preferential attachment – Wikipedia]

'''

[[algo-resource-allocation]]
==== algo.resourceAllocation

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.resourceAllocation`
| Category | Link Prediction
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.resourceAllocation(node [, relTypes, direction, cutoff])
YIELD node1, node2, score
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `node` | Vertex | Yes | — | Source vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `cutoff` | Double | No | `0.0` | Minimum score threshold
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | RID | Source vertex identity
| `node2` | RID | Compared vertex identity
| `score` | Double | Resource allocation score
|===

*Description*

Computes the resource allocation link prediction score: `score = Σ 1/degree(w)` over all common neighbors w. Models the fraction of "resources" that each common neighbor can send to both endpoints if it distributes equally to all its neighbors. Conceptually similar to Adamic-Adar but uses degree directly rather than its logarithm.

*Use Cases*

- Link prediction with emphasis on rare intermediaries
- Recommendation in sparse networks
- Metabolic network pathway prediction

*Example*

[source,cypher]
----
MATCH (p:Protein {id:'P1'})
CALL algo.resourceAllocation(p, 'INTERACTS', 'BOTH', 0.1)
YIELD node1, node2, score
RETURN node2, score ORDER BY score DESC LIMIT 10
----

*References*

- Zhou, T., Lü, L., & Zhang, Y. C. (2009). Predicting missing links via local information. _European Physical Journal B_, 71(4), 623–630.

'''

[[algo-sim-rank]]
==== algo.simRank

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.simRank`
| Category | Similarity
| Complexity | CPU+RAM
| Min Args | 2
| Max Args | 5
|===

*Syntax*

[source,cypher]
----
CALL algo.simRank(nodeA, nodeB [, relTypes, decayFactor, maxIterations])
YIELD similarity, nodeAId, nodeBId
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `nodeA` | Vertex | Yes | — | First vertex
| `nodeB` | Vertex | Yes | — | Second vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `decayFactor` | Double | No | `0.8` | Decay constant C ∈ (0,1)
| `maxIterations` | Integer | No | `5` | Number of refinement iterations
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `similarity` | Double | SimRank similarity score between nodeA and nodeB
| `nodeAId` | RID | Identity of nodeA
| `nodeBId` | RID | Identity of nodeB
|===

*Description*

Computes SimRank similarity between two vertices using the recursive definition: two nodes are similar if they are pointed to by similar nodes. Maintains a full V×V similarity matrix (O(V²) memory), updated iteratively. Similarity of a node with itself is 1.0. Uses IN adjacency for the standard directed graph formulation.

*Use Cases*

- Structural equivalence analysis in directed graphs
- Computing vertex pair similarity for matching problems
- Entity resolution in knowledge graphs

*Example*

[source,cypher]
----
MATCH (a:Paper {id:'P1'}), (b:Paper {id:'P2'})
CALL algo.simRank(a, b, 'CITES', 0.8, 5)
YIELD similarity, nodeAId, nodeBId
RETURN nodeAId, nodeBId, similarity
----

*References*

- Jeh, G. & Widom, J. (2002). SimRank: a measure of structural-context similarity. _KDD 2002_.

'''

[[algo-knn]]
==== algo.knn

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.knn`
| Category | Similarity
| Complexity | CPU
| Min Args | 0
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.knn([k, relTypes, direction]) YIELD node1, node2, similarity
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `k` | Integer | `5` | Number of nearest neighbours to return per node
| `relTypes` | String | all types | Comma-separated edge type names to consider
| `direction` | String | `"BOTH"` | Edge traversal direction: `IN`, `OUT`, or `BOTH`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | Vertex | Source vertex
| `node2` | Vertex | One of the k nearest neighbours of node1
| `similarity` | Float | Jaccard similarity score in [0.0, 1.0]
|===

*Description*

Constructs a k-Nearest Neighbours (KNN) graph using Jaccard similarity on neighbourhood sets. For each pair of nodes, the similarity is computed as `|N(u) ∩ N(v)| / |N(u) ∪ N(v)|`. Each node retains only its top-k neighbours by similarity score. This produces a new graph representing approximate structural proximity, useful as a pre-processing step for community detection or recommendation.

*Use Cases*

- Constructing similarity graphs for downstream community detection
- Collaborative filtering and recommendation
- Detecting structurally similar entities (users, documents, products)

*Example*

[source,cypher]
----
CALL algo.knn(10, 'KNOWS', 'BOTH')
YIELD node1, node2, similarity
RETURN node1.name AS source, node2.name AS neighbour, similarity
ORDER BY source, similarity DESC
----

*References*

- https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm[k-nearest neighbors algorithm – Wikipedia]

'''

[[algo-same-community]]
==== algo.sameCommunity

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.sameCommunity`
| Category | Link Prediction
| Complexity | CPU
| Min Args | 3
| Max Args | 3
|===

*Syntax*

[source,cypher]
----
CALL algo.sameCommunity(node1, node2, communityProperty) YIELD node1, node2, coefficient
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `node1` | Vertex | — | First vertex
| `node2` | Vertex | — | Second vertex
| `communityProperty` | String | — | Property name holding the community identifier on each vertex
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | Vertex | The first vertex
| `node2` | Vertex | The second vertex
| `coefficient` | Float | `1.0` if both nodes share the same community value, `0.0` otherwise
|===

*Description*

A simple link-prediction feature: returns `1.0` when both nodes carry the same value for the given community property and `0.0` otherwise. Useful as a binary feature in machine-learning pipelines or to filter / weight edges based on community membership.

*Use Cases*

- Feature engineering for link-prediction models
- Filtering candidate edges to intra-community connections
- Evaluating community detection quality (ground-truth comparison)

*Example*

[source,cypher]
----
MATCH (a:Person), (b:Person)
WHERE a <> b
CALL algo.sameCommunity(a, b, 'community')
YIELD coefficient
WHERE coefficient = 1.0
RETURN a.name, b.name
----

*References*

- https://en.wikipedia.org/wiki/Link_prediction[Link prediction – Wikipedia]

'''

[[algo-total-neighbors]]
==== algo.totalNeighbors

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.totalNeighbors`
| Category | Link Prediction
| Complexity | CPU
| Min Args | 2
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.totalNeighbors(node1, node2 [, relTypes, direction]) YIELD node1, node2, coefficient
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `node1` | Vertex | — | First vertex
| `node2` | Vertex | — | Second vertex
| `relTypes` | String | all types | Comma-separated edge type names to traverse
| `direction` | String | `"BOTH"` | Edge traversal direction: `IN`, `OUT`, or `BOTH`
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node1` | Vertex | The first vertex
| `node2` | Vertex | The second vertex
| `coefficient` | Long | Size of the union of the two neighbourhood sets
|===

*Description*

Computes `|N(u) ∪ N(v)|` — the total number of distinct neighbours across both nodes. This is the denominator of Jaccard similarity and is used as a link-prediction feature measuring total neighbourhood reach. Calculated as `|N(u)| + |N(v)| − |N(u) ∩ N(v)|` using a BitSet intersection.

*Use Cases*

- Feature engineering for link-prediction models
- Identifying node pairs with large combined reach
- Pre-filtering candidates before more expensive similarity computation

*Example*

[source,cypher]
----
MATCH (a:Person {name: 'Alice'}), (b:Person {name: 'Bob'})
CALL algo.totalNeighbors(a, b, 'KNOWS', 'BOTH')
YIELD coefficient
RETURN coefficient AS totalNeighbours
----

*References*

- https://en.wikipedia.org/wiki/Jaccard_index[Jaccard index – Wikipedia]

'''

=== Network Flow

[[algo-max-flow]]
==== algo.maxFlow

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.maxFlow`
| Category | Network Flow
| Complexity | CPU+RAM
| Min Args | 2
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.maxFlow(sourceNode, sinkNode [, relTypes, capacityProperty])
YIELD maxFlow, sourceId, sinkId
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `sourceNode` | Vertex | Yes | — | Flow source vertex
| `sinkNode` | Vertex | Yes | — | Flow sink vertex
| `relTypes` | String | No | all types | Comma-separated relationship types
| `capacityProperty` | String | No | uniform capacity | Edge property for flow capacity
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `maxFlow` | Double | Maximum flow value from source to sink
| `sourceId` | RID | Identity of the source vertex
| `sinkId` | RID | Identity of the sink vertex
|===

*Description*

Computes the maximum flow from a source to a sink using the Edmonds-Karp algorithm (BFS-based Ford-Fulkerson). Time complexity is O(V·E²). Builds a V×V capacity matrix (O(V²) memory). Returns a single row with the maximum achievable flow.

*Use Cases*

- Network throughput capacity analysis
- Traffic flow optimization
- Supply chain max throughput calculation

*Example*

[source,cypher]
----
MATCH (src:Node {id:'source'}), (sink:Node {id:'sink'})
CALL algo.maxFlow(src, sink, 'PIPE', 'capacity')
YIELD maxFlow, sourceId, sinkId
RETURN maxFlow, sourceId, sinkId
----

*References*

- https://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm[Edmonds-Karp algorithm – Wikipedia]

'''

=== Traversal and Sampling

[[algo-random-walk]]
==== algo.randomWalk

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.randomWalk`
| Category | Traversal / Sampling
| Complexity | CPU
| Min Args | 2
| Max Args | 5
|===

*Syntax*

[source,cypher]
----
CALL algo.randomWalk(startNode, steps [, relTypes, direction, seed])
YIELD path, steps
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `startNode` | Vertex | Yes | — | Starting vertex
| `steps` | Integer | Yes | — | Maximum number of steps to walk
| `relTypes` | String | No | all types | Comma-separated relationship types
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `seed` | Long | No | current time | Random seed for reproducibility
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `path` | List<RID> | Ordered list of vertex identities visited
| `steps` | Integer | Actual number of steps taken (may be less than requested if dead-end reached)
|===

*Description*

Performs a single random walk starting from the given vertex. At each step, one neighbor is chosen uniformly at random from available edges matching the direction and type constraints. The walk stops when the requested number of steps is reached or a dead-end (no valid neighbors) is encountered.

*Use Cases*

- Graph embedding feature generation (node2vec-style)
- Sampling large graphs for approximate analysis
- Stochastic exploration of graph neighborhoods

*Example*

[source,cypher]
----
MATCH (start:Page {id:'home'})
CALL algo.randomWalk(start, 20, 'LINKS_TO', 'OUT', 42)
YIELD path, steps
RETURN path, steps
----

*References*

- https://en.wikipedia.org/wiki/Random_walk[Random walk – Wikipedia]

'''

=== Network Science Metrics

[[algo-assortativity]]
==== algo.assortativity

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.assortativity`
| Category | Network Science
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.assortativity([relTypes])
YIELD assortativity, edgeCount
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `assortativity` | Double | Newman's degree assortativity coefficient ∈ [-1, 1]
| `edgeCount` | Integer | Number of edges used in the computation
|===

*Description*

Computes Newman's degree assortativity coefficient. A positive value indicates that high-degree nodes tend to connect to other high-degree nodes (assortative); negative indicates disassortative mixing.

*Use Cases*

- Network topology characterization
- Social network analysis (social networks are typically assortative)
- Comparing mixing patterns across different networks

*Example*

[source,cypher]
----
CALL algo.assortativity('KNOWS')
YIELD assortativity, edgeCount
RETURN assortativity, edgeCount
----

*References*

- Newman, M. E. J. (2002). Assortative mixing in networks. _Physical Review Letters_, 89(20), 208701.

'''

[[algo-rich-club]]
==== algo.richClub

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.richClub`
| Category | Network Science
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.richClub([relTypes, minDegree])
YIELD degree, richClubCoefficient, nodeCount, edgeCount
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types
| `minDegree` | Integer | No | `2` | Starting degree threshold
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `degree` | Integer | Degree threshold k for this row
| `richClubCoefficient` | Double | φ(k): density of edges among nodes with degree > k
| `nodeCount` | Integer | Number of nodes with degree > k
| `edgeCount` | Integer | Number of edges among those nodes
|===

*Description*

Computes the rich-club coefficient φ(k) for every degree threshold k from `minDegree` to the maximum degree in the graph. One row is returned per degree threshold. Formula: `φ(k) = 2·E_k / (N_k·(N_k−1))`, where N_k = number of nodes with degree > k and E_k = edges between those nodes.

*Use Cases*

- Characterizing the "rich get richer" phenomenon
- Network topology analysis: do high-degree nodes interconnect?
- Comparing network structure across domains

*Example*

[source,cypher]
----
CALL algo.richClub('KNOWS', 2)
YIELD degree, richClubCoefficient, nodeCount, edgeCount
RETURN degree, richClubCoefficient ORDER BY degree ASC
----

*References*

- Colizza, V., Flammini, A., Serrano, M. A., & Vespignani, A. (2006). Detecting rich-club ordering in complex networks. _Nature Physics_, 2, 110–115.

'''

[[algo-influence-maximization]]
==== algo.influenceMaximization

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.influenceMaximization`
| Category | Network Science
| Complexity | CPU
| Min Args | 1
| Max Args | 4
|===

*Syntax*

[source,cypher]
----
CALL algo.influenceMaximization(k [, relTypes, simulations, propagationProbability])
YIELD nodeId, rank, marginalGain
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `k` | Integer | Yes | — | Number of seed nodes to select
| `relTypes` | String | No | all types | Comma-separated relationship types
| `simulations` | Integer | No | `100` | Monte Carlo simulation count per candidate
| `propagationProbability` | Double | No | `0.1` | Independent Cascade activation probability per edge
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeId` | RID | Vertex identity of the selected seed node
| `rank` | Integer | Selection rank (1 = best seed)
| `marginalGain` | Double | Expected additional nodes activated by adding this seed
|===

*Description*

Selects k seed nodes that maximize influence spread using a greedy algorithm with Monte Carlo simulation of the Independent Cascade (IC) model. In each round, the candidate that maximizes the expected spread when added to the current seed set is selected.

*Use Cases*

- Viral marketing campaign planning
- Identifying optimal vaccination targets for epidemic control
- Information cascade maximization in social media

*Example*

[source,cypher]
----
CALL algo.influenceMaximization(3, 'FOLLOWS', 200, 0.15)
YIELD nodeId, rank, marginalGain
RETURN nodeId, rank, marginalGain ORDER BY rank ASC
----

*References*

- Kempe, D., Kleinberg, J., & Tardos, E. (2003). Maximizing the spread of influence through a social network. _KDD 2003_.

'''

=== Community Quality Metrics

[[algo-modularity-score]]
==== algo.modularityScore

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.modularityScore`
| Category | Community Quality
| Complexity | CPU
| Min Args | 1
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.modularityScore(communityProperty [, relTypes])
YIELD modularity, communities, edgeCount
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `communityProperty` | String | Yes | — | Vertex property name holding the community label
| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `modularity` | Double | Newman-Girvan modularity Q ∈ [-0.5, 1]
| `communities` | Integer | Number of distinct communities found
| `edgeCount` | Long | Total number of edges in the graph
|===

*Description*

Evaluates the modularity of an existing community partition stored as a vertex property. Formula: `Q = Σ_c [ L_c/m − (d_c/(2m))² ]`, where L_c is the number of internal edges in community c, d_c is the sum of degrees in c, and m is the total number of edges. Returns a single row.

*Use Cases*

- Evaluating the output of community detection algorithms
- Comparing alternative community assignments
- Graph partition quality benchmarking

*Example*

[source,cypher]
----
CALL algo.modularityScore('community', 'KNOWS')
YIELD modularity, communities, edgeCount
RETURN modularity, communities, edgeCount
----

*References*

- Newman, M. E. J. & Girvan, M. (2004). Finding and evaluating community structure in networks. _Physical Review E_, 69(2), 026113.

'''

[[algo-conductance]]
==== algo.conductance

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.conductance`
| Category | Community Quality
| Complexity | CPU
| Min Args | 1
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.conductance(communityProperty [, relTypes])
YIELD community, conductance, internalEdges, boundaryEdges, nodeCount
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `communityProperty` | String | Yes | — | Vertex property name holding the community label
| `relTypes` | String | No | all types | Comma-separated relationship types
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `community` | Object | Community label value
| `conductance` | Double | Conductance score (lower = better community)
| `internalEdges` | Integer | Edges with both endpoints inside the community
| `boundaryEdges` | Integer | Edges crossing the community boundary
| `nodeCount` | Integer | Number of nodes in the community
|===

*Description*

Computes the conductance of each community in an existing partition stored as a vertex property. Formula: `conductance = cut(C) / min(vol(C), vol(V\C))`, where `cut(C)` is the number of boundary edges and `vol(C)` is the sum of degrees within C. Lower conductance indicates a better-separated community. Returns one row per community.

*Use Cases*

- Evaluating community separation quality
- Identifying poorly-separated or "leaky" communities
- Graph cut analysis for partitioning algorithms

*Example*

[source,cypher]
----
CALL algo.conductance('community', 'KNOWS')
YIELD community, conductance, internalEdges, boundaryEdges, nodeCount
RETURN community, conductance ORDER BY conductance ASC
----

*References*

- https://en.wikipedia.org/wiki/Conductance_(graph)[Conductance (graph) – Wikipedia]

'''

=== Graph Statistics

[[algo-graph-summary]]
==== algo.graphSummary

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.graphSummary`
| Category | Graph Statistics
| Complexity | CPU
| Min Args | 0
| Max Args | 2
|===

*Syntax*

[source,cypher]
----
CALL algo.graphSummary([relTypes, nodeLabels])
YIELD nodeCount, edgeCount, avgDegree, maxDegree, minDegree, density, isolatedNodes, selfLoops
----

*Parameters*

[cols="2,1,1,2,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `relTypes` | String | No | all types | Comma-separated relationship types to include
| `nodeLabels` | String | No | all labels | Comma-separated node labels to include
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `nodeCount` | Integer | Total number of vertices
| `edgeCount` | Long | Total number of edges
| `avgDegree` | Double | Average degree across all vertices
| `maxDegree` | Integer | Maximum vertex degree
| `minDegree` | Integer | Minimum vertex degree
| `density` | Double | Graph density: `2E / (V × (V-1))`
| `isolatedNodes` | Integer | Number of vertices with degree 0
| `selfLoops` | Long | Number of self-loop edges
|===

*Description*

Computes a comprehensive one-row statistical summary of the graph. Density is computed as `2E / (V × (V-1))` for undirected graphs (0 for graphs with ≤ 1 node). Self-loops are counted separately. Optionally filters by edge type and/or node label.

*Use Cases*

- Quick graph health check and characterization
- Monitoring graph evolution over time
- Input validation before running expensive algorithms

*Example*

[source,cypher]
----
CALL algo.graphSummary('KNOWS', 'Person')
YIELD nodeCount, edgeCount, avgDegree, maxDegree, density, isolatedNodes
RETURN nodeCount, edgeCount, avgDegree, maxDegree, density, isolatedNodes
----

'''

=== Node Embedding

Node embedding procedures map each vertex to a dense real-valued vector that encodes its structural position in the graph. The resulting vectors can be used directly as features for machine-learning tasks (classification, clustering, link prediction) or for similarity search using ArcadeDB's vector index.

All embedding procedures yield a `List<Float>` that can be stored as a vertex property or forwarded to a downstream vector index.

[[algo-fastrp]]
==== algo.fastrp

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.fastrp`
| Category | Node Embedding
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.fastrp([config]) YIELD node, embedding
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `dimensions` | Integer | `128` | Embedding vector size
| `iterations` | Integer | `4` | Number of propagation rounds
| `normalization` | Float | `0.0` | Degree-normalisation exponent α: weight of neighbour j contributing to node i is proportional to `deg(i)^{-α} × deg(j)^{-α}`; 0 = no normalisation, 1 = GCN-style
| `selfInfluence` | Float | `0.0` | Weight [0,1] given to the node's own previous embedding vs. the aggregated neighbour embedding
| `relTypes` | String | all types | Comma-separated edge type names
| `direction` | String | `BOTH` | Edge traversal direction: `IN`, `OUT`, or `BOTH`
| `seed` | Long | `-1` | Random seed; -1 = random
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `embedding` | List<Float> | L2-normalised embedding vector of length `dimensions`
|===

*Description*

FastRP (Fast Random Projection, Chen et al. 2019) generates dense node embeddings without any training phase. Each node is initialised with a sparse ternary random vector (values ±√3 with probability 1/6 each, 0 with probability 2/3 — the optimal sparse random projection of Achlioptas 2003). The embedding is then iteratively refined by computing a weighted average of neighbour embeddings, followed by L2 normalisation. The resulting vectors capture multi-hop structural proximity and run orders of magnitude faster than walk-based methods.

*Use Cases*

- Fast baseline embeddings for downstream ML pipelines
- Similarity search via vector index
- Graph-aware feature generation without labelled data

*Example*

[source,cypher]
----
CALL algo.fastrp({dimensions: 64, iterations: 3, seed: 42})
YIELD node, embedding
RETURN node.name AS name, embedding
----

*References*

- https://arxiv.org/abs/1908.11512[Chen, H. et al. (2019). Fast and Accurate Network Embeddings via Very Sparse Random Projection. _CIKM 2019_]

'''

[[algo-node2vec]]
==== algo.node2vec

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.node2vec`
| Category | Node Embedding
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.node2vec([config]) YIELD node, embedding
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `embeddingDimension` | Integer | `128` | Embedding vector size
| `walkLength` | Integer | `80` | Number of steps per random walk
| `walksPerNode` | Integer | `10` | Random walks generated per node
| `iterations` | Integer | `1` | Training epochs over all walks
| `windowSize` | Integer | `10` | Skip-gram context window radius
| `negSamples` | Integer | `5` | Negative samples per positive (centre, context) pair
| `learningRate` | Float | `0.025` | Initial SGD learning rate (linearly decayed per epoch)
| `p` | Float | `1.0` | Return parameter: high p → less likely to return to previous node
| `q` | Float | `1.0` | In-out parameter: low q → DFS-like exploration; high q → BFS-like
| `relTypes` | String | all types | Comma-separated edge type names
| `direction` | String | `BOTH` | Edge traversal direction
| `seed` | Long | `-1` | Random seed; -1 = random
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `embedding` | List<Float> | L2-normalised embedding vector of length `embeddingDimension`
|===

*Description*

Node2Vec (Grover & Leskovec, 2016) learns node embeddings using biased second-order random walks combined with a Skip-gram model trained via negative sampling. The walk bias is controlled by two parameters:

- *p* (return parameter): low p makes the walk more likely to revisit the previous node (BFS-like neighbourhood exploration)
- *q* (in-out parameter): low q makes the walk explore outward (DFS-like), revealing structural roles; high q stays close to the source (community-based embeddings)

The Skip-gram model is trained with SGD and negative sampling, using a linearly decaying learning rate per epoch.

*Use Cases*

- Community-aware embeddings (high q)
- Structural-role embeddings (low q)
- Link prediction feature generation
- Node classification pre-training

*Example*

[source,cypher]
----
CALL algo.node2vec({embeddingDimension: 64, p: 1.0, q: 0.5, walkLength: 30, seed: 42})
YIELD node, embedding
RETURN node.name AS name, embedding
----

*References*

- https://arxiv.org/abs/1607.00653[Grover, A. & Leskovec, J. (2016). node2vec: Scalable Feature Learning for Networks. _KDD 2016_]

'''

[[algo-graphsage]]
==== algo.graphsage

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.graphsage`
| Category | Node Embedding
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.graphsage([config]) YIELD node, embedding
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `embeddingDimension` | Integer | `64` | Output embedding size per layer
| `layers` | Integer | `2` | Number of aggregation layers (receptive field = layers hops)
| `relTypes` | String | all types | Comma-separated edge type names
| `direction` | String | `BOTH` | Edge traversal direction
| `seed` | Long | `-1` | Random seed; -1 = random
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `embedding` | List<Float> | L2-normalised embedding vector of length `embeddingDimension`
|===

*Description*

GraphSAGE (Hamilton et al., 2017) — inductive neighbourhood aggregation. This implementation is *unsupervised*: no training labels are needed. Node features are initialised from structural properties (log-normalised degree + Gaussian noise) and then propagated through `layers` rounds of mean aggregation, each followed by a random linear projection (Xavier initialisation) and ReLU activation. The resulting embeddings capture multi-hop structural similarity.

Because the projection matrices are randomly initialised rather than trained, the embeddings are structurally consistent (nodes with similar neighbourhoods receive similar embeddings) and can be used directly as ML features or refined with downstream fine-tuning.

*Use Cases*

- Inductive embeddings for graphs that grow over time
- Feature generation for node classification / link prediction
- Similarity search and clustering on graph data

*Example*

[source,cypher]
----
CALL algo.graphsage({embeddingDimension: 32, layers: 2, seed: 7})
YIELD node, embedding
RETURN node.name AS name, embedding
----

*References*

- https://arxiv.org/abs/1706.02216[Hamilton, W. et al. (2017). Inductive Representation Learning on Large Graphs. _NeurIPS 2017_]

'''

[[algo-hashgnn]]
==== algo.hashgnn

[cols="1,3", options="header"]
|===
| Property | Value

| Procedure | `algo.hashgnn`
| Category | Node Embedding
| Complexity | CPU
| Min Args | 0
| Max Args | 1
|===

*Syntax*

[source,cypher]
----
CALL algo.hashgnn([config]) YIELD node, embedding
----

*Parameters*

[cols="2,1,1,3", options="header"]
|===
| Name | Type | Default | Description

| `config` | Map | `{}` | Configuration map (see below)
|===

*Config Parameters*

[cols="2,1,1,3", options="header"]
|===
| Key | Type | Default | Description

| `embeddingDimension` | Integer | `128` | Output embedding size (number of MinHash functions)
| `iterations` | Integer | `4` | Message-passing rounds (receptive field = iterations hops)
| `relTypes` | String | all types | Comma-separated edge type names
| `direction` | String | `BOTH` | Edge traversal direction
| `seed` | Long | `-1` | Random seed; -1 = random
|===

*Yield Fields*

[cols="2,1,3", options="header"]
|===
| Field | Type | Description

| `node` | Vertex | The vertex
| `embedding` | List<Float> | L2-normalised MinHash embedding vector of length `embeddingDimension`
|===

*Description*

HashGNN is a *training-free* graph neural network that uses locality-sensitive hashing for neighbourhood aggregation. Each node is initialised with a sparse random binary feature vector (≈12.5% density) derived from its structural identity. For each propagation round, each node's feature set is expanded by OR-combining neighbour feature sets, then reduced to a fixed-size MinHash sketch using random linear hash functions `h_d(x) = (a·x + b) mod F`. The final embedding is the L2-normalised MinHash signature, providing probabilistic Jaccard similarity guarantees: two nodes with similar MinHash signatures have similar neighbourhood feature sets.

HashGNN is extremely fast (no matrix multiplication, no gradient computation) and naturally handles graphs without node features.

*Use Cases*

- Ultra-fast structural embeddings as features for downstream ML
- Graph-level similarity via pooled embeddings
- Anomaly detection (unusual neighbourhood structure)

*Example*

[source,cypher]
----
CALL algo.hashgnn({embeddingDimension: 64, iterations: 3, seed: 42})
YIELD node, embedding
RETURN node.name AS name, embedding
----

*References*

- https://arxiv.org/abs/2206.01652[Dasoulas, G. et al. (2022). Improving Graph Neural Networks with Learnable Propagation Operators. _ICML 2022_]
- https://en.wikipedia.org/wiki/MinHash[MinHash – Wikipedia]

'''

=== SQL Path Functions

The following algorithms are also available as SQL functions that return a list of vertex RIDs (identities). They can be used in any SQL `SELECT` statement.

[[algo-dijkstra-sql]]
==== dijkstra() SQL Function

[cols="1,3", options="header"]
|===
| Property | Value

| Function | `dijkstra()`
| Category | Path Finding
| Complexity | CPU
|===

*Syntax*

[source,sql]
----
dijkstra(<sourceVertex>, <destinationVertex>, <weightEdgeFieldName> [, <direction>])
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `sourceVertex` | Vertex / RID | Yes | — | Source vertex
| `destinationVertex` | Vertex / RID | Yes | — | Destination vertex
| `weightEdgeFieldName` | String | Yes | — | Edge property name for weights
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Return Value*

`List<RID>` — Ordered list of vertex identities on the shortest path (including source and destination).

*Description*

SQL function equivalent of `algo.dijkstra`. Internally delegates to the A* implementation with no heuristic. Returns a `LinkedList` of vertex RIDs.

*Example*

[source,sql]
----
SELECT dijkstra(#12:0, #12:5, 'weight', 'OUT') AS path
----

See also: <<dijkstra,dijkstra()>> in SQL Functions reference and <<algo-dijkstra>> for the Cypher procedure variant.

'''

[[algo-bellman-ford-sql]]
==== bellmanFord() SQL Function

[cols="1,3", options="header"]
|===
| Property | Value

| Function | `bellmanFord()`
| Category | Path Finding
| Complexity | CPU
|===

*Syntax*

[source,sql]
----
bellmanFord(<sourceVertex>, <destinationVertex>, <weightEdgeFieldName> [, <direction>])
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `sourceVertex` | Vertex / RID | Yes | — | Source vertex
| `destinationVertex` | Vertex / RID | Yes | — | Destination vertex
| `weightEdgeFieldName` | String | Yes | — | Edge property name for weights (may be negative)
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
|===

*Return Value*

`List<RID>` — Ordered list of vertex identities on the shortest path.

*Description*

SQL function equivalent of `algo.bellmanford`. Supports negative edge weights and detects negative-weight cycles (returns an empty path if a negative cycle is reachable).

*Example*

[source,sql]
----
SELECT bellmanFord(#12:0, #12:5, 'cost') AS path
----

See also: <<algo-bellman-ford>> for the Cypher procedure variant.

'''

[[algo-shortest-path-sql]]
==== shortestPath() SQL Function

[cols="1,3", options="header"]
|===
| Property | Value

| Function | `shortestPath()`
| Category | Path Finding
| Complexity | CPU
|===

*Syntax*

[source,sql]
----
shortestPath(<sourceVertex>, <destinationVertex> [, <direction> [, <edgeType>]])
----

*Parameters*

[cols="2,1,1,1,3", options="header"]
|===
| Parameter | Type | Required | Default | Description

| `sourceVertex` | Vertex / RID | Yes | — | Source vertex
| `destinationVertex` | Vertex / RID | Yes | — | Destination vertex
| `direction` | String | No | `"BOTH"` | Traversal direction: `"IN"`, `"OUT"`, or `"BOTH"`
| `edgeType` | String | No | all types | Edge type name to restrict traversal
|===

*Return Value*

`List<RID>` — Ordered list of vertex identities on the shortest path (unweighted; minimizes hop count).

*Description*

Finds the unweighted shortest path (minimum hop count) between two vertices using bidirectional BFS (meets in the middle), which can be significantly faster than single-direction BFS for large graphs. Optionally accepts a map parameter with `maxDepth` and `edge` flags.

*Example*

[source,sql]
----
SELECT shortestPath(#12:0, #12:9, 'BOTH', 'KNOWS') AS path
----

*References*

- https://en.wikipedia.org/wiki/Bidirectional_search[Bidirectional search – Wikipedia]

See also: <<shortest-path-function,shortestPath()>> in SQL Functions reference.

'''

=== Notes

==== relTypes Parameter

All procedures that accept a `relTypes` parameter use comma-separated relationship type names. To traverse *all* relationship types, pass an empty string `''` or omit the parameter where it is optional:

[source,cypher]
----
CALL algo.pagerank()               -- all types (parameter omitted)
CALL algo.wcc('')                  -- all types (empty string)
CALL algo.wcc('KNOWS,FOLLOWS')     -- only KNOWS and FOLLOWS edges
----

==== direction Parameter

Where supported, the `direction` parameter controls which edges are considered:

- `"OUT"` — only outgoing edges from each vertex
- `"IN"` — only incoming edges to each vertex
- `"BOTH"` — edges in either direction (default for most algorithms)

==== Config Map Parameters

Several algorithms (PageRank, Louvain, Betweenness, LabelPropagation) accept an optional configuration map as their first argument:

[source,cypher]
----
CALL algo.pagerank({dampingFactor: 0.9, maxIterations: 50})
YIELD node, score
----

==== Memory Considerations

Algorithms marked *CPU+RAM* build in-memory data structures that scale with V² or E²:

[cols="3,3", options="header"]
|===
| Algorithm | Memory Usage

| `algo.apsp` | O(V²) distance matrix
| `algo.maxFlow` | O(V²) capacity matrix
| `algo.kShortestPaths` | O(V²) weight matrix
| `algo.simRank` | O(V²) similarity matrix
| `algo.hierarchicalClustering` | O(V²) similarity pairs
|===

For graphs with more than a few thousand vertices, these algorithms may require significant heap space.
